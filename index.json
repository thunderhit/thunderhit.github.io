[{"authors":["大邓"],"categories":null,"content":"","date":1593455400,"expirydate":-62135596800,"kind":"taxonomy","lang":"zh","lastmod":1593455400,"objectID":"27bdb5faec546bfee57db3cf7ca86930","permalink":"https://thunderhit.github.io/author/%E5%A4%A7%E9%82%93/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%A4%A7%E9%82%93/","section":"authors","summary":"","tags":null,"title":"大邓","type":"authors"},{"authors":["大邓"],"categories":["培训","课程"],"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n","date":1593455400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1593455400,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://thunderhit.github.io/talk/example/","publishdate":"2020-06-06T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"模块化课程设计，兼顾理论与实战","tags":["网络爬虫","文本分析","机器学习"],"title":"Python＆Stata应用能力提升与实证前沿云特训","type":"talk"},{"authors":["大邓"],"categories":["教程","视频"],"content":"目录    N级标题   This is an H1  This is an H2  This is an H3   加粗、斜体 高亮 下划线 删除线 引用 列表  有序列表 无序列表   代码块 表格 超链接 插入图片 任务清单 数学公式 mermaid图表 支持     学习编程的过程需要敲大量代码，遇到很多错误，好脑子不如烂笔头，能一边敲代码一边做笔记，学起来事倍功半，今天分享大家一个做笔记的工具软件Typora。\n N级标题 一个#就是一个级，最多支持六级标题。\n# This is an H1 ## This is an H2 ### This is an H3  效果如下\nThis is an H1 This is an H2 This is an H3 \n加粗、斜体 **加粗的文本**  加粗的文本\n*斜体文本*  斜体文本\n\n高亮 ==highlight==  ==highlight==\n\n下划线 \u0026lt;u\u0026gt;下划线内容\u0026lt;/u\u0026gt;  下划线内容\n\n删除线  ~~Mistaken text.~~  Mistaken text.\n\n引用 \u0026gt; This is another blockquote with one paragraph. There is three empty line to seperate two blockquote.  效果如下\n This is another blockquote with one paragraph. There is three empty line to seperate two blockquote.\n \n列表 有序列表 ordered list: 1. Red 2. Green 3.\tBlue  ordered list:\n  Red\n  Green\n  Blue\n   注意.之后留一个空格再书写内容   无序列表 un-ordered list: - Red - Green - Blue  效果如下\nun-ordered list:\n  Red\n  Green\n  Blue\n   注意.之后同样留一个空格再书写内容   \n代码块 一般情况下，word等office软件不支持代码高亮，且存储代码容易乱行。以typora为首的markdown语法完美支持各种代码，这里以Python代码为例\n​```python import requests url = 'https://www.baidu.com/' resp = requests.get(url) ​```  效果如下\nimport requests url = 'https://www.baidu.com/' resp = requests.get(url)  \n表格 | First Column | Second Column | | ------------- | ------------- | | Content Cell1 | Content Cell2 | | Content Cell3 | Content Cell4 |  效果\n   First Column Second Column     Content Cell1 Content Cell2   Content Cell3 Content Cell4    \n超链接 [百度](https://www.baidu.com/)   百度\n\n插入图片 ![](图片文件路径或网址) ![](https://thunderhit.github.io/post/hugo/featured_hu9a15b8275a5635099198061a5e2dc1dd_3029870_720x0_resize_lanczos_2.png)  效果如下\n\n任务清单 某日任务清单 - [x] 6点起床 - [ ] 步数达到10000步 - [x] 读一小时书 - [x] 日消费不超过50元  效果\n 6点起床 步数达到10000步 读一小时书 日消费不超过50元  \n数学公式 在markdown中用$夹住Latex公式表达式\n$\\lim_{x \\to \\infty} \\exp(-x) = 0$  效果如下\n$\\lim_{x \\to \\infty} \\exp(-x) = 0$\n\nmermaid图表 Typora中可以绘制流程图、序列图、状态图、甘特图、饼形图、类图等，这里以流程图为例\n​```mermaid graph LR A--\u0026gt;B B--\u0026gt;C C--\u0026gt;D ​```  效果\ngraph LR A--\u0026gt;B B--\u0026gt;C C--\u0026gt;D  更多mermaid可以看\n在Markdown中用mermaid语法绘制图表\n\n支持 分享不易，谢谢大家点赞分享和红包^_^\n","date":1592561547,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1592561547,"objectID":"0b4a6f22772b7e48647d3184b533cd14","permalink":"https://thunderhit.github.io/post/typora%E6%9C%80%E5%A5%BD%E7%94%A8%E7%9A%84%E5%85%8D%E8%B4%B9markdown%E8%BD%AF%E4%BB%B6/","publishdate":"2020-06-19T18:12:27+08:00","relpermalink":"/post/typora%E6%9C%80%E5%A5%BD%E7%94%A8%E7%9A%84%E5%85%8D%E8%B4%B9markdown%E8%BD%AF%E4%BB%B6/","section":"post","summary":"照着视频学markdown","tags":["markdown","typora","文字工具","笔记"],"title":"半小时学会Markdown标记语法","type":"post"},{"authors":["大邓"],"categories":["视频","教程"],"content":"目录  Mac环境配置  软件包下载 一、Python安装  命令行打开的方法   二、pip3设置  2.1 更改pip3镜像 2.2 使用方法   三、Jupyter notebook  3.1 安装 3.2 调用 3.3 常用快捷键   四、Tips 支持     Mac环境配置 软件包下载  链接:https://pan.baidu.com/s/1tbgGBcAnYSMZXp80F0nM1Q 密码:t307    一、Python安装 官网 https://www.python.org/\n mac自带python2，为了与python2区别，凡是在命令行中使用pip和python，我们都要加上3。   安装成功的标准是==命令行可以调用python3==\n$ python3  \n命令行打开的方法 ==command+空格== 启动 ==聚焦搜索Spotlight==，再输入terminal\n\n二、pip3设置 pip3是python的命令行安装工具，可以帮我们安装第三方库。\n2.1 更改pip3镜像 为了保证安装的速度和成功率，命令行执行\npip3 config set global.index-url https://mirrors.aliyun.com/pypi/simple/  2.2 使用方法 pip3 install packagename  \n三、Jupyter notebook 3.1 安装 命令行执行\npip3 install jupyter  3.2 调用 命令行执行\njupyter notebook  3.3 常用快捷键    jupyter内快捷键 功能     ESC+A（ESC+B） 当前单元格上(下)新建一个新的Cell   D+D 删除当前单元格   Shift+Enter 执行单元格内的Python代码   ESC+M 单元格由代码模式转为Markdown标记模式     Markdown语法特别好用，强烈建议学习，顺便安装一个Typora软件   \n四、Tips  环境配置太难，而且有时候电脑还会出现一些视频里出现不了的问题。这时不妨在==淘宝==搜python环境配置，寻找一对一远程协助   支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1592556092,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1592556092,"objectID":"7f2fae8e06e89e140f5266fee695b286","permalink":"https://thunderhit.github.io/post/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEmac/","publishdate":"2020-06-19T16:41:32+08:00","relpermalink":"/post/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEmac/","section":"post","summary":"照着视频配置Mac","tags":["环境配置","python"],"title":"Mac电脑Python环境配置","type":"post"},{"authors":["大邓"],"categories":["视频","教程"],"content":"目录  Win环境配置  软件包下载 一、Python安装  安装注意事项   二、pip配置  2.1 更改pip镜像 2.2 使用方法   三、Jupyter notebook  3.1 安装 3.2 调用 3.3 常用快捷键   四、Tips 支持     Win环境配置 软件包下载  链接:https://pan.baidu.com/s/1tbgGBcAnYSMZXp80F0nM1Q 密码:t307    一、Python安装 官网 https://www.python.org/\n安装注意事项  推荐选择3.7.5. 最新的bug比较多 ==选择Install Now默认安装方式== 勾选Add Python 3.x to PATH，这样命令行可以调用python  \n二、pip配置 pip是python的命令行安装工具，可以帮我们安装第三方库。\n2.1 更改pip镜像 为了保证安装的速度和成功率，命令行执行\npip config set global.index-url https://mirrors.aliyun.com/pypi/simple/  2.2 使用方法 pip install packagename  \n三、Jupyter notebook 3.1 安装 命令行执行\npip install jupyter  3.2 调用 命令行执行\njupyter notebook  3.3 常用快捷键    jupyter内快捷键 功能     ESC+A（ESC+B） 当前单元格上(下)新建一个新的Cell   D+D 删除当前单元格   Shift+Enter 执行单元格内的Python代码   ESC+M 单元格由代码模式转为标记模式     Markdown语法特别好用，强烈建议学习，顺便安装一个Typora软件   \n四、Tips  环境配置太难，而且有时候电脑还会出现一些视频里出现不了的问题。这时不妨在==淘宝==搜python环境配置，寻找一对一远程协助   支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1592555951,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1592555951,"objectID":"563f986b9e73ebcedfd9d94463071563","permalink":"https://thunderhit.github.io/post/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEwin/","publishdate":"2020-06-19T16:39:11+08:00","relpermalink":"/post/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEwin/","section":"post","summary":"照着视频配置Windows","tags":["环境配置","python"],"title":"Windows电脑Python环境配置","type":"post"},{"authors":["大邓"],"categories":["教程","生产力工具"],"content":"Mermaid可以用文本方式绘制图表和流程图，相比Visio而言更加的轻量便捷，此外Markdown内部支持Mermaid语法，可以有效避免切换软件，让我们更加专注于内容本身。\n mermaid官方文档\n目录  图表类型支持 饼形图 流程图 时序图 状态图 甘特图 class类图  支持     图表类型支持   饼形图(Pie Chart)\n  流程图（Flow Chart）\n  时序图（Sequence Diagram）\n  状态图(State Diagram)\n  甘特图（Gantt Diagram）\n  类图(class Diagram)\n  等等\n饼形图 饼形图是我们经常用到的图表，在mermaid中最简单，基本上一看就会\npie title Pets adopted by volunteers \u0026quot;Dogs\u0026quot; : 386 \u0026quot;Cats\u0026quot; : 85 \u0026quot;Rats\u0026quot; : 15  代码\n​```mermaid pie title Pets adopted by volunteers \u0026quot;Dogs\u0026quot; : 386 \u0026quot;Cats\u0026quot; : 85 \u0026quot;Rats\u0026quot; : 15 ​```  用到的关键词\n   关键词 功能     pie 定义饼形图   title 标题    \n  流程图 graph TD A--\u0026gt;B A--\u0026gt;C B--\u0026gt;D C--\u0026gt;D  该流程图就是用下方的代码再markdown中实现的\n​```mermaid graph RL; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D; ​```  关键词解读\n   关键词 功能     graph 定义流程图   TD 流程图方向。mermai的方位还有T、D、L、 R，分别代表上、下、左、右。两个方位组合成一个流动方向。本案例是从上到下，即TD   --\u0026gt; 有向箭头    节点还可以用:::调用修饰函数，如下\ngraph LR A:::someclass --\u0026gt; B classDef someclass fill:#f96;  ​```mermaid graph LR A:::someclass --\u0026gt; B classDef someclass fill:#f96; ​```  \n时序图 时序图用于描述对象之间的传递消息的时间顺序, 即用例中的行为顺序.\nsequenceDiagram participant Alice participant Bob participant John Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  顺序图稍微复杂了一丢丢，代码如下\n​```mermaid sequenceDiagram participant Alice participant Bob participant John Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ​```  用到的关键词\n   关键词 功能     sequenceDiagram 定义顺序表   participant 定义图中的节点   loop 、end 循环体代码块，以loop开头，end结束；   Note 提示框   right of 方位关键词   -\u0026gt;\u0026gt; 实线箭头连接线   --\u0026gt;\u0026gt; 虚线箭头    \n状态图 通过建立对象的生存周期模型来描述对象随时间变化的动态行为\nstateDiagram Start --\u0026gt; First First --\u0026gt; Second First --\u0026gt; Third Second --\u0026gt; End Third --\u0026gt; End state First { [*] --\u0026gt; fir fir --\u0026gt; [*] } state Second { [*] --\u0026gt; sec sec --\u0026gt; [*] } state Third {}  代码\n​```mermaid stateDiagram Start --\u0026gt; First First --\u0026gt; Second First --\u0026gt; Third Second --\u0026gt; End Third --\u0026gt; End state First { [*] --\u0026gt; fir fir --\u0026gt; [*] } state Second { [*] --\u0026gt; sec sec --\u0026gt; [*] } state Third {} ​```  代码关键词解读\n   关键词 功能     stateDiagram 用于定义状态图   [*] 实心黑点   --\u0026gt; 有向实线   state 用于定义状态    我们可以看到状态state还可以定义内部的流程，如First和Second；Third没有定义内部处理过程。\n\n甘特图 gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d  代码如下\n​```mermaid gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d ​```  用到的关键词\n   关键词 功能     gantt 定义甘特图   dataFormat 定义日期格式   title 标题   excludes 排除项目周期中的放假休息等日期   section 定义一个项目   :done 、 :active、: 项目中的状态   after 紧随其后    \nclass类图 面向对象的编程会经常看到类，类与类有所属关系。比如中国人是人类的一员，而人类又隶属于灵长类动物。\nclassDiagram Animal \u0026lt;|-- Duck Animal \u0026lt;|-- Fish Animal \u0026lt;|-- Zebra Animal : int age Animal : String gender Animal: isMammal() Animal: mate() class Duck{ String beakColor swim() quack() } class Fish{ int sizeInFeet canEat() } class Zebra{ bool is_wild run() eat() }  代码\n​```mermaid classDiagram Animal \u0026lt;|-- Duck Animal \u0026lt;|-- Fish Animal \u0026lt;|-- Zebra Animal : int age Animal : String gender Animal: isMammal() Animal: mate() class Duck{ String beakColor swim() quack() } class Fish{ int sizeInFeet canEat() } class Zebra{ bool is_wild run() eat() } ​```  用到的关键词\n   关键词 功能     classDiagram 定义类图   \u0026lt;-- 隶属于某类   Animal : int age 定义Animal的年龄属性(属性没有用括号)   Animal: isMammal() 定义Animal的是否为哺乳动物方法(方法有括号)   class Duck 定义Duck类    大家如果熟悉Python，就能理解类的属性和方法区别就是是否有括号。\n\n支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1592455503,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1592455503,"objectID":"15ebc3278e244efc0b8ef20aa02716c8","permalink":"https://thunderhit.github.io/post/mermaid/","publishdate":"2020-06-18T12:45:03+08:00","relpermalink":"/post/mermaid/","section":"post","summary":"支持饼形图、流程图、类图、状态图、甘特图等","tags":["markdown","mermaid","流程图"],"title":"在Markdown中用mermaid语法绘制图表","type":"post"},{"authors":["大邓"],"categories":["Python","视频课"],"content":" Python数据挖掘\n与文本分析 \n 按Space键查看PPT\n   地点: 小鹅通云直播 时间: 2020年6月29号 ~ 7月2号\n   大数据时代，\n大家会遇到两大难题:  1.如何批量快速获取数据\n 2.如何处理分析非结构(多媒体)数据\n  这门Python课解决了:  1.入门Python语法\n 2.高效采集网络数据\n 3.非结构数据清洗与分析\n  课程亮点  1. 秉承20%时间学到80%常用易用知识\n 2. 理论+实战，每章节均穿插实战案例\n  Python有什么优点？  1.语法简单\n 2.功能强大\n 3.免费开源\n  语法简洁  Python是一种\u0026quot;说人话\u0026quot;的编程\u0026quot;语言\u0026rdquo;  \n  功能强大  1.Web网站开发\n 2.自动化办公(运营)\n 3.数据分析\n 4.机器学习AI\n 4.游戏开发\n 5.等等   Python培训\n含四大模块:  打通数据处理分析全流程\n  1.Python语法入门\n2.网络爬虫(数据采集)\n3.文本(数据)分析\n4.机器学习与文本分析\n  报名咨询方式  培训有对应的录播课，\n点击下方蓝色链接直接购买\n  Python网络爬虫与文本分析\n Thanks  ","date":1592352000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1592352000,"objectID":"4e994214cd4c2e62510290a987ccfd1e","permalink":"https://thunderhit.github.io/slides/web_spider_and_text_mining/","publishdate":"2020-06-17T00:00:00Z","relpermalink":"/slides/web_spider_and_text_mining/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":["网络爬虫","文本分析","机器学习","Python语法"],"title":"《Python数据挖掘与文本分析》培训介绍","type":"slides"},{"authors":["大邓"],"categories":["教程","视频"],"content":"在B站看到一位博主用Hugo制作个人博客的视频，感觉挺简单的，真的十几分钟就能看到云端出现自己的博客，当然了想让自己的博客更美观更炫酷，精雕细琢会花很多功夫。现在大家看到的效果，大邓用了一整天的时间，一点点修饰改动出来的。\n目录  1. 安装Hugo 2. 新建Hugo项目 3. Academic主题下载 4. 启动本地博客 5. 在本地新建一篇文章 6. 将本地博客部署到服务器 更多 实验代码下载 打赏   1. 安装Hugo 这里以Mac为例，安装Hugo，命令行输入\nbrew install hugo  \n2. 新建Hugo项目 切换到桌面(我喜欢把项目放到桌面), 命令行执行\ncd desktop  新建一个叫做MyBlog的hugo项目文件夹，命令行执行\nhugo new site MyBlog  现在可以在桌面看到一个MyBlog文件夹，接下来切换工作目录到MyBlog\ncd MyBlog  记者目前我们的命令行处于MyBlog的根目录 , 接下来下载网站主题\n\n3. Academic主题下载 Hugo有很多主题，我选择的 https://themes.gohugo.io/academic/，\n在命令行逐行执行下方命令\ncd themes git clone https://github.com/gcushen/hugo-academic.git   我们可以在 MyBlog/themes 看到多了一个 hugo-academic文件夹，把hugo-academic改为academic ，现在网站已经建立好了~    这里切换回项目根目录MyBlog   cd ..  命令行执行pwd，检查一下目录\npwd  得到\n/Users/电脑用户名/desktop/MyBlog  4. 启动本地博客 现在我们以academic主题为例，启动博客\n命令行执行\nhugo server -t academic --buildDrafts   补充: t的意思是主题   执行后，在命令行中会提示我们\nhttp://localhost:1313/  在浏览器中复制粘贴上方的链接，我们的Blog毛坯房搭建好了~\n5. 在本地新建一篇文章 依旧是MyBlog根目录，命令行执行\nhugo new post/first-article.md   在MyBlog/content内新生成了一个post文件夹，并且post内有了一个first-article.md文件。   接下来就是在first-article.md内用markdown方式写内容即可。\n我们测试一下现在的网站,继续回到MyBlog根目录，命令行执行\nhugo server -t academic --buildDrafts  在浏览器中我们可以看到有First Ariticle的文章。\n6. 将本地博客部署到服务器 在github新建一个仓库，仓库名命名方式\n\u0026lt;你的github用户名\u0026gt;.github.io  比如我的github账号名是thunderhit，那么仓库名为\nthunderhit.github.io  在MyBlog根目录，命令行执行\nhugo --theme=academic --baseUrl='https://thunderhit.github.io/' --buildDrafts   补充: 主题academic, 网站地址 https://thunderhit.github.io/ ，你们根据自己需要改成自己的仓库名   现在我们在MyBlog中多了一个public文件夹，其中有我们新建的文章内容。\n绑定public与github仓库\n命令行切换到public目录，初始化git\ncd public git init git add . git commit -m '我的hugo博客第一次提交'  把public与远程github仓库关联\n依次执行（大家的github地址略微不同，需要改动一下)\ngit remote add origin git@github.com:thunderhit/thunderhit.github.io.git git push -u origin master  命令行上传完毕后，在浏览器网址栏打开链接 https://thunderhit.github.io/\n就可以看到我们自己的博客了~\n\n更多 如果大家想学仔细学Hugo，推荐大家看B站Up主：ianianying的视频\n \n实验代码下载 如果实验没成功，大家可以下载我的博客项目。项目资源获取方式，【公众号：大邓和他的Python】后台回复关键词\u0026rdquo;Hugo\u0026rdquo;\n\n打赏 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1592299880,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1592299880,"objectID":"1ba1da42e133e1a006844b4b012bf188","permalink":"https://thunderhit.github.io/post/hugo/","publishdate":"2020-06-16T17:31:20+08:00","relpermalink":"/post/hugo/","section":"post","summary":"适用于个人博客场景；学习难度适中，我用了一上午搭建好了一个漂亮的Blog","tags":["建站"],"title":"使用Hugo框架建立个人网站","type":"post"},{"authors":["大邓"],"categories":["教程","python库","视频"],"content":"目录  cntopic 安装 使用 1. 读取文件 2. 准备数据 3. 训练lda模型 4. 使用LDA模型  4.1 准备document 4.2 预测document对应的话题 4.3 显示每种话题与对应的特征词之间关系 4.4 话题分布情况 4.5 可视化（功能不稳定）   五、存储与导入lda模型  如果 更多 支持     cntopic 简单好用的lda话题模型，支持中英文。该库基于gensim和pyLDAvis，实现了lda话题模型及可视化功能。\n \n安装 pip install cntopic  \n使用 这里给大家引入一个场景，假设大家采集新闻数据，忘记采集新闻文本对应的新闻类别，如果人工标注又很费工夫。这时候我们可以用lda话题模型帮我们洞察数据中的规律，发现新闻有n种话题群体。这样lda模型对数据自动打标注topic_1, topic_2, topic_3\u0026hellip; ,topic_n。\n我们研究者的工作量仅仅限于解读topic_1, topic_2, topic_3\u0026hellip; ,topic_n分别是什么话题即可。\nlda训练过程，大致分为\n 读取文件 准备数据 训练lda模型 使用lda模型 存储与导入lda模型  \n1. 读取文件 这里我们用一个新闻数据,一共有10类，每类1000条数据，涵盖\n\u0026lsquo;时尚\u0026rsquo;, \u0026lsquo;财经\u0026rsquo;, \u0026lsquo;科技\u0026rsquo;, \u0026lsquo;教育\u0026rsquo;, \u0026lsquo;家居\u0026rsquo;, \u0026lsquo;体育\u0026rsquo;, \u0026lsquo;时政\u0026rsquo;, \u0026lsquo;游戏\u0026rsquo;, \u0026lsquo;房产\u0026rsquo;, \u0026lsquo;娱乐\u0026rsquo;\nimport pandas as pd df = pd.read_csv('chinese_news.csv') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  label content     0 体育 鲍勃库西奖归谁属？ NCAA最强控卫是坎巴还是弗神新浪体育讯如今，本赛季的NCAA进入到了末...   1 体育 麦基砍28+18+5却充满寂寞 纪录之夜他的痛阿联最懂新浪体育讯上天对每个人都是公平的，贾维...   2 体育 黄蜂vs湖人首发：科比冲击七连胜 火箭两旧将登场新浪体育讯北京时间3月28日，NBA常规赛洛...   3 体育 双面谢亚龙作秀终成做作 谁来为低劣行政能力埋单是谁任命了谢亚龙？谁放纵了谢亚龙？谁又该为谢亚...   4 体育 兔年首战山西换帅后有虎胆 张学文用乔丹名言励志今晚客场挑战浙江稠州银行队，是山西汾酒男篮的兔...     label标签的分布情况\ndf['label'].value_counts()  家居 1000 时尚 1000 房产 1000 时政 1000 教育 1000 游戏 1000 财经 1000 娱乐 1000 体育 1000 科技 1000 Name: label, dtype: int64  \n2. 准备数据 一般准备数据包括:\n 分词、数据清洗 按照模块需求整理数据的格式  注意在scikit-learn中:\n 英文文本不需要分词，原封不动传入即可。 中文文本需要先分词，后整理为英文那样用空格间隔的字符串。形如”我 爱 中国“  import jieba def text2tokens(raw_text): #将文本raw_text分词后得到词语列表 tokens = jieba.lcut(raw_text) #tokens = raw_text.lower().split(' ') #英文用空格分词即可 tokens = [t for t in tokens if len(t)\u0026gt;1] #剔除单字 return tokens #对content列中所有的文本依次进行分词 documents = [text2tokens(txt) for txt in df['content']] #显示前5个document print(documents[:5])  [['鲍勃', '库西', '奖归', 'NCAA', '最强', '控卫', '坎巴', '还是', '弗神', '新浪', '体育讯', '称赞', '得分', '能力', '毋庸置疑',...], ['球员', '大东', '赛区', '锦标赛', '全国', '锦标赛', '他场', '27.1', '6.1', '篮板', '5.1', '助攻',..], ['依旧', '如此', '给力', '疯狂', '表现', '开始', '这个', '赛季', '疯狂', '表现', '结束', '这个', '赛季', '我们', '全国', '锦标赛', '前进', '并且', '之前', '曾经', '连赢', '赢得', '大东', ...], ['赛区', '锦标赛', '冠军', '这些', '归功于', '坎巴', '沃克', '康涅狄格', '大学', '主教练', '吉姆', '卡洪', ...], ['称赞', '一名', '纯正', '控卫', '而且', '能为', '我们', '得分', '单场', '42', '有过', '单场', '17', '助攻', ...]]  \n3. 训练lda模型 现在开始正式使用cntopic模块，开启LDA话题模型分析。步骤包括\n   Step 功能 代码     0 准备documents，已经在前面准备好了 -   1 初始化Topic类 topic = Topic(cwd=os.getcwd())   2 根据documents数据，构建词典空间 topic.create_dictionary(documents=documents)   3 构建语料(将文本转为文档-词频矩阵) topic.create_corpus(documents=documents)   4 指定n_topics，构建LDA话题模型 topic.train_lda_model(n_topics)    这里我们就按照n_topics=10构建lda话题模型，一般情况n_topics可能要实验多次，找到最佳的n_topics\n运行过程中会在代码所在的文件夹内生成一个output文件夹，内部含有\n dictionary.dict 词典文件 lda.model.xxx 多个lda模型文件，其中xxx是代指  上述代码耗时较长，请耐心等待程序运行完毕~\nimport os from cntopic import Topic topic = Topic(cwd=os.getcwd()) #构建词典dictionary topic.create_dictionary(documents=documents) #根据documents数据，构建词典空间 topic.create_corpus(documents=documents) #构建语料(将文本转为文档-词频矩阵) topic.train_lda_model(n_topics=10) #指定n_topics，构建LDA话题模型  \u0026lt;gensim.models.ldamulticore.LdaMulticore at 0x158da5090\u0026gt;  \n4. 使用LDA模型 上面的代码大概运行了5分钟，LDA模型已经训练好了。\n现在我们可以利用LDA做一些事情，包括\n   Step 功能 代码 补充     1 分词后的某文档 document = [\u0026lsquo;游戏\u0026rsquo;, \u0026lsquo;体育\u0026rsquo;]    2 预测document对应的话题 topic.get_document_topics(document)    3 显示每种话题与对应的特征词之间关系 topic.show_topics()    4 数据中不同话题分布情况 topic.topic_distribution(raw_documents) raw_documents是列表或series，如本教程中的df[\u0026lsquo;content\u0026rsquo;]   5 可视化LDA话题模型（功能不稳定） topic.visualize_lda() 可视化结果在output中查找vis.html文件，浏览器打开即可    4.1 准备document 假设有一个文档 '游戏体育真有意思' 分词处理得到document\ndocument = jieba.lcut('游戏体育真有意思') document  ['游戏', '体育', '真', '有意思']  4.2 预测document对应的话题 我们使用topic模型，看看document对应的话题\ntopic.get_document_topics(document)  [(0, 0.02501536), (1, 0.025016038), (2, 0.28541195), (3, 0.025018401), (4, 0.025018891), (5, 0.025017735), (6, 0.51443774), (7, 0.02502284), (8, 0.025015472), (9, 0.025025582)]  我们的lda话题模型是按照n_topics=10训练的，限制调用topic预测某个document时，得到的结果是这10种话题及对应概率的元组列表。\n从中可以看到概率最大的是 话题6， 概率有0.51443774。\n所以我们可以大致认为document是话题6\n4.3 显示每种话题与对应的特征词之间关系 但是仅仅告诉每个文档是 话题n，我们仍然不知道 话题n代表的是什么，所以我们需要看看每种 话题n对应的 特征词语。\ntopic.show_topics()  [(0, '0.042*\u0026quot;基金\u0026quot; + 0.013*\u0026quot;市场\u0026quot; + 0.011*\u0026quot;投资\u0026quot; + 0.009*\u0026quot;公司\u0026quot; + 0.005*\u0026quot;上涨\u0026quot; + 0.004*\u0026quot;股票\u0026quot; + 0.004*\u0026quot;房地产\u0026quot; + 0.004*\u0026quot;指数\u0026quot; + 0.004*\u0026quot;房价\u0026quot; + 0.004*\u0026quot;2008\u0026quot;'), (1, '0.010*\u0026quot;中国\u0026quot; + 0.007*\u0026quot;移民\u0026quot; + 0.006*\u0026quot;项目\u0026quot; + 0.005*\u0026quot;发展\u0026quot; + 0.005*\u0026quot;表示\u0026quot; + 0.005*\u0026quot;经济\u0026quot; + 0.005*\u0026quot;政府\u0026quot; + 0.005*\u0026quot;土地\u0026quot; + 0.004*\u0026quot;政策\u0026quot; + 0.004*\u0026quot;问题\u0026quot;'), (2, '0.014*\u0026quot;比赛\u0026quot; + 0.009*\u0026quot;他们\u0026quot; + 0.008*\u0026quot;球队\u0026quot; + 0.007*\u0026quot;篮板\u0026quot; + 0.006*\u0026quot;我们\u0026quot; + 0.005*\u0026quot;球员\u0026quot; + 0.005*\u0026quot;季后赛\u0026quot; + 0.005*\u0026quot;时间\u0026quot; + 0.005*\u0026quot;热火\u0026quot; + 0.005*\u0026quot;赛季\u0026quot;'), (3, '0.013*\u0026quot;我们\u0026quot; + 0.013*\u0026quot;一个\u0026quot; + 0.009*\u0026quot;自己\u0026quot; + 0.009*\u0026quot;这个\u0026quot; + 0.007*\u0026quot;没有\u0026quot; + 0.007*\u0026quot;他们\u0026quot; + 0.006*\u0026quot;可以\u0026quot; + 0.006*\u0026quot;就是\u0026quot; + 0.006*\u0026quot;很多\u0026quot; + 0.006*\u0026quot;记者\u0026quot;'), (4, '0.020*\u0026quot;电影\u0026quot; + 0.010*\u0026quot;导演\u0026quot; + 0.009*\u0026quot;微博\u0026quot; + 0.008*\u0026quot;影片\u0026quot; + 0.006*\u0026quot;观众\u0026quot; + 0.006*\u0026quot;一个\u0026quot; + 0.005*\u0026quot;自己\u0026quot; + 0.005*\u0026quot;票房\u0026quot; + 0.004*\u0026quot;拍摄\u0026quot; + 0.004*\u0026quot;娱乐\u0026quot;'), (5, '0.018*\u0026quot;学生\u0026quot; + 0.015*\u0026quot;留学\u0026quot; + 0.008*\u0026quot;大学\u0026quot; + 0.008*\u0026quot;可以\u0026quot; + 0.006*\u0026quot;功能\u0026quot; + 0.006*\u0026quot;像素\u0026quot; + 0.006*\u0026quot;拍摄\u0026quot; + 0.006*\u0026quot;采用\u0026quot; + 0.005*\u0026quot;学校\u0026quot; + 0.005*\u0026quot;申请\u0026quot;'), (6, '0.007*\u0026quot;玩家\u0026quot; + 0.006*\u0026quot;封神\u0026quot; + 0.006*\u0026quot;手机\u0026quot; + 0.006*\u0026quot;online\u0026quot; + 0.006*\u0026quot;the\u0026quot; + 0.006*\u0026quot;游戏\u0026quot; + 0.005*\u0026quot;陈水扁\u0026quot; + 0.005*\u0026quot;活动\u0026quot; + 0.005*\u0026quot;to\u0026quot; + 0.005*\u0026quot;一个\u0026quot;'), (7, '0.009*\u0026quot;信息\u0026quot; + 0.009*\u0026quot;考试\u0026quot; + 0.009*\u0026quot;游戏\u0026quot; + 0.007*\u0026quot;工作\u0026quot; + 0.007*\u0026quot;手机\u0026quot; + 0.006*\u0026quot;四六级\u0026quot; + 0.006*\u0026quot;考生\u0026quot; + 0.005*\u0026quot;发展\u0026quot; + 0.004*\u0026quot;可以\u0026quot; + 0.004*\u0026quot;霸王\u0026quot;'), (8, '0.015*\u0026quot;我们\u0026quot; + 0.011*\u0026quot;企业\u0026quot; + 0.011*\u0026quot;产品\u0026quot; + 0.010*\u0026quot;市场\u0026quot; + 0.009*\u0026quot;家具\u0026quot; + 0.009*\u0026quot;品牌\u0026quot; + 0.008*\u0026quot;消费者\u0026quot; + 0.007*\u0026quot;行业\u0026quot; + 0.007*\u0026quot;中国\u0026quot; + 0.007*\u0026quot;一个\u0026quot;'), (9, '0.012*\u0026quot;游戏\u0026quot; + 0.011*\u0026quot;玩家\u0026quot; + 0.010*\u0026quot;可以\u0026quot; + 0.008*\u0026quot;搭配\u0026quot; + 0.008*\u0026quot;活动\u0026quot; + 0.006*\u0026quot;时尚\u0026quot; + 0.005*\u0026quot;OL\u0026quot; + 0.004*\u0026quot;获得\u0026quot; + 0.004*\u0026quot;任务\u0026quot; + 0.004*\u0026quot;手机\u0026quot;')]  根据上面的 话题n 与 特征词 大致可以解读每个 话题n 是什么内容的话题。\n4.4 话题分布情况 现在我们想知道数据集中不同 话题n 的分布情况\ntopic.topic_distribution(raw_documents=df['content'])  9 1670 1 1443 0 1318 5 1265 4 1015 2 970 8 911 3 865 7 307 6 236 Name: topic, dtype: int64  我们的数据有10类，每类是1000条。而现在LDA话题模型单纯的根据文本的一些线索，按照n_topics=10给我们分出的效果还不错。\n最完美的情况是每个 话题n 都是接近1000, 现在 话题9太多， 话题6、 话题7太少。\n不过我们也要注意到某些话题可能存在交集，容易分错，比如\n 财经、房产、时政 体育娱乐 财经、科技  等\n综上，目前模型还算可以，表现还能接受。\n4.5 可视化（功能不稳定） 现在只有10个话题， 我们用肉眼看还能接受，但是当话题数太多的时，还是借助可视化工具帮助我们科学评判训练结果。\n这就用到topic.visualize_lda()，\ntopic.visualize_lda()  运行结束后在\n代码所在的文件夹output文件夹中找vis.html文件，右键浏览器打开。\n可视化功能不稳定，存在vis.html打不开的情况；希望海涵\n图中有左右两大区域\n 左侧 话题分布情况，圆形越大话题越多，圆形四散在四个象限 右侧 某话题对应的特征词，从上到下权重越来越低  需要注意的是左侧\n 尽量圆形均匀分布在四个象限比较好，如果圆形全部集中到有限的区域，模型训练不好 圆形与圆形交集较少比较好，如果交集太多，说明n_topics设置的太大，应该设置的再小一些  \n五、存储与导入lda模型 lda话题模型训练特别慢，如果不保存训练好的模型，实际上是在浪费我们的生命和电脑计算力。\n好消息是cntopic默认为大家存储模型，存储地址是output文件夹内，大家只需要知道如何导入模型即可。\n这里需要导入的有两个模型，使用步骤\n   步骤 模型 代码 作用     0 - - 准备documents   1 - topic = Topic(cwd=os.getcwd()) 初始化   2 词典 topic.load_dictionary(dictpath='output/dictionary.dict\u0026rsquo;) 直接导入词典，省略topic.create_dictionary()   3 - topic.create_corpus(documents=documents) 构建语料(将文本转为文档-词频矩阵)   4 lda话题模型 topic.load_lda_model(modelpath='output/model/lda.model\u0026rsquo;) 导入lda话题模型， 相当于省略topic.train_lda_model(n_topics)    现在我们试一试, 为了与之前的区分，这里我们起名topic2\ntopic2 = Topic(cwd=os.getcwd()) topic2.load_dictionary(dictpath='output/dictionary.dict') topic2.create_corpus(documents=documents) topic2.load_lda_model(modelpath='output/model/lda.model')  大家可以自己回去试一试第4部分使用LDA模型的相关功能\n\n如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n\n更多   B站 公众号：大邓和他的python  知乎  github  \n支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1592213480,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1592213480,"objectID":"b7c9cd53480049c9711c7393bb24e6fe","permalink":"https://thunderhit.github.io/post/cntopic/","publishdate":"2020-06-15T17:31:20+08:00","relpermalink":"/post/cntopic/","section":"post","summary":"简单好用的lda话题模型，支持中英文。该库基于gensim和pyLDAvis，实现了lda话题模型及可视化功能。","tags":["python库","文本分析","机器学习","LDA话题模型"],"title":"cntopic: 中文LDA话题模型","type":"post"},{"authors":null,"categories":["培训"],"content":"\n在过去的两年间，Python一路高歌猛进，成功窜上“最火编程语言”的宝座。惊奇的是使用Python最多的人群其实不是程序员，而是数据科学家，尤其是社会科学家，涵盖的学科有经济学、管理学、会计学、社会学、传播学、新闻学等等。\n大数据时代到来，网络数据正成为潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于网页中。非计算机专业背景的人也可借助机器学习、人工智能等方法进行研究。使用网络世界数据进行研究，面临两大难点：\n 数据的批量获取 文本（非结构化）数据的处理与分析  参照已发表的社科类文章，希望帮大家解决这两大难点。课程设计的初衷是用最少的时间让大家学到最有用最常用最易用的知识点，降低学习难度。\n课程目录 第一节 课程简介  课程介绍 课程知识点分布情况\n  第二节 环境配置  Mac环境配置 Windows环境配置 pip安装问题解决办法 jupyter notebook使用方法\n  第三节 python基本语法  python跟英文一样也是一门语言，这很文科 字符串 列表 元组 字典 集合 if 条件语句 for循环语句 try-except异常处理语句\n  第四节 python高级语法  切片-对想要的数据字段进行切片 列表推导式 函数 csv文件存储库 os文件路径操作库 re正则表达式(文本分析利器) python初学者常见错误\n  第五节 网络爬虫原理  理解访问与请求 寻求网址规律 requests访问库 pyquery网页解析定位库\n  第六节 网络爬虫实战  静态网站-天涯论坛 静态网站-大众点评 静态网站-boss直聘 动态网站-百度企业信用 动态网站-京东评论 动态网站-B站弹幕 动态网站-B站评论 如何用pandas采集网页中的表格数据\n  第七节 初识文本分析  如何从不同格式的文件中读取数据 jieba分词、词频统计与可视化 海量公司年报的情感分析(中文) 英文数据的情感分析 如何对excel、csv文件做数据分析(pandas数据分析库)\n  第八节 文本分析与机器学习  机器学习概论 用机器学习做文本分析的步骤 机器学习库scikit-learn 文本特征工程(描述数据的方式) 在线评论情感分类 了解聚类Kmeans算法 文本相似度计算 LDA话题模型 计算消费者异质性(特征向量) 文本分析在经管研究中的应用案例\n  购买方式 1. 腾讯课堂-录播课  大邓个人开课，不能开票报销  2. 小鹅通-直播课  机构合作，可支持开票报销 2020年6月29日~7月2日   3. 现场工作坊  机构合作，可支持开票报销 地点杭州，由于疫情原因，开展不了现场教学，时间待定~  ","date":1591747200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1591747200,"objectID":"5ddf2550046d35c7ae62e41be85f7603","permalink":"https://thunderhit.github.io/courses/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/","publishdate":"2020-06-10T00:00:00Z","relpermalink":"/courses/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/","section":"courses","summary":"学完本课，您将学会如何获取及处理大数据","tags":["python","网络爬虫","机器学习","文本分析"],"title":"Python网络爬虫与文本分析","type":"courses"},{"authors":null,"categories":["培训"],"content":"\n很多职场人士想提高办公效率，python是一门很神奇的工具，ta可以帮我们职场人士做很多事情，尤其是在自动化办公领域，批量自动处理文件简直是职场人士的福音。自动化办公场景包括 excel、ppt、word等文件处理、邮件自动发送、网络爬虫（如数据采集、批量文件下载），这次我就来理一理 python 自动化办公的那些知识点。\n课程目录 准备篇  想象力丰富的自动化场景 快速上手一个小案例  简单文件处理篇  批量更改文件名 批量检索pdf、word 批量读取csv、excel 定制excel文件内单元格的格式 批量将txt汇总到一个excel中  网络爬虫  网络爬虫原理 你的第一个爬虫 自动下载某网站的pdf 自动下载某网站的图片 自动下载某网站视频  报告报表自动化  批量生成合同(word) 网店口碑可视化（词云图制作）  邮件自动化  如何设置邮箱，让Python控制你的邮箱(163邮箱为例) 月底给员工自动群发工资条邮件  生成图形界面篇  简单的图形界面设计 自动群发邮件软件的可视化窗口实现  购买方式 腾讯课堂-Python自动化办公实战 ","date":1591747200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1591747200,"objectID":"09fca2c1615a4ff646334a745fd838ad","permalink":"https://thunderhit.github.io/courses/python%E8%87%AA%E5%8A%A8%E5%8C%96%E5%8A%9E%E5%85%AC/","publishdate":"2020-06-10T00:00:00Z","relpermalink":"/courses/python%E8%87%AA%E5%8A%A8%E5%8C%96%E5%8A%9E%E5%85%AC/","section":"courses","summary":"自动群发邮件、自动生成分析报告、数据采集等","tags":["python","网络爬虫","自动化办公","文本分析"],"title":"Python自动化办公实战","type":"courses"},{"authors":["大邓"],"categories":["教程","python库","视频"],"content":"目录  一、文本事理类型分析    事件图谱（事理图谱）的类型     二、安装方法 三、使用    3.1 主函数 3.2 统计   如果 更多 支持     一、文本事理类型分析 中文复合事件抽取，可以用来识别文本的模式，包括条件事件、顺承事件、反转事件。\n我仅仅是对代码做了简单的修改，增加了函数说明注释和stats函数，可以用于统计文本中各种模式的分布(数量)情况。代码原作者为刘焕勇 https://github.com/liuhuanyong\n事件图谱（事理图谱）的类型 项目地址https://github.com/liuhuanyong/ComplexEventExtraction 项目介绍很详细，感兴趣的一定要去原项目看一下。\n   事件 含义 形式化 事件应用 图谱场景 举例     条件事件 某事件条件下另一事件发生 如果A那么B 事件预警 时机判定 \u0026lt;限制放宽,立即增产\u0026gt;   反转事件 某事件与另一事件形成对立 虽然A但是B 预防不测 反面教材 \u0026lt;起步晚,发展快\u0026gt;   顺承事件 某事件紧接着另一事件发生 A接着B 事件演化 未来意图识别 \u0026lt;去旅游,买火车票\u0026gt;    分析出文本中的条件、顺承、反转，理论上就可以构建知识网络(本库做不到这可视化)。 1、反转事件图谱 2、条件事件图谱 二、安装方法 pip install eventextraction  三、使用 3.1 主函数 from eventextraction import EventsExtraction extractor = EventsExtraction() content = '虽然你做了坏事，但我觉得你是好人。一旦时机成熟，就坚决推行' datas = extractor.extract_main(content) print(datas)  运行结果\n[{'sent': '虽然你做了坏事，但我觉得你是好人', 'type': 'but', 'tuples': {'pre_wd': '虽然', 'pre_part': '你做了坏事，', 'post_wd': '但', 'post_part ': '我觉得你是好人'}}, {'sent': '一旦时机成熟，就坚决推行', 'type': 'condition', 'tuples': {'pre_wd': '一旦', 'pre_part': '时机成熟，', 'post_wd': '就', 'post_part ': '坚决推行'}}]  3.2 统计 from eventextraction import EventsExtraction extractor = EventsExtraction() content = '虽然你做了坏事，但我觉得你是好人。一旦时机成熟，就坚决推行' datas = extractor.extract_main(content) print(extractor.stats(datas))  运行结果\n{'but': 1, 'condition': 1, 'seq': 0, 'more': 0, 'other': 0}  如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n更多   B站 公众号：大邓和他的python  知乎 [github](https://github.com/thunderhit）  支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1591093880,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1591093880,"objectID":"c7ad46d985720e696bcc5ceacea64a8e","permalink":"https://thunderhit.github.io/post/eventextraction/","publishdate":"2020-06-02T18:31:20+08:00","relpermalink":"/post/eventextraction/","section":"post","summary":"快速构建不同领域(手机、汽车等)的情感词典","tags":["python库","文本分析","事件抽取"],"title":"eventextraction: 快速构建不同领域(手机、汽车等)的情感词典","type":"post"},{"authors":["大邓"],"categories":["教程","python库","视频"],"content":"目录  pdfdocx    github项目地址 安装 使用 拆开pdfdocx   如果 更多 支持     最近运行课件代码，发现pdf文件读取部分的函数失效。这里找到读取pdf文件的可运行代码，为了方便后续学习使用，我已将pdf和docx读取方法封装成pdfdocx包。\npdfdocx github项目地址 只有简单的两个读取函数\n read_pdf(file) read_docx(file)  file为文件路径，函数运行后返回file文件内的文本数据。\n安装 pip install pdfdocx  使用 读取pdf文件\nfrom pdfdocx import read_pdf p_text = read_pdf('test/data.pdf') print(p_text)  Run\n这是来⾃pdf⽂件内的内容  from pdfdocx import read_docx d_text = read_pdf('test/data.docx') print(d_text)  Run\n这是来⾃docx⽂件内的内容  拆开pdfdocx 希望大家能安装好，如果安装或者使用失败，可以使用下面的代码作为备选方法。虽然繁琐，能用就好。\n读取pdf\nfrom io import StringIO from pdfminer.converter import TextConverter from pdfminer.layout import LAParams from pdfminer.pdfdocument import PDFDocument from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter from pdfminer.pdfpage import PDFPage from pdfminer.pdfparser import PDFParser import re def read_pdf(file): \u0026quot;\u0026quot;\u0026quot; 读取pdf文件，并返回其中的文本内容 :param file: pdf文件路径 :return: docx中的文本内容 \u0026quot;\u0026quot;\u0026quot; output_string = StringIO() with open(file, 'rb') as in_file: parser = PDFParser(in_file) doc = PDFDocument(parser) rsrcmgr = PDFResourceManager() device = TextConverter(rsrcmgr, output_string, laparams=LAParams()) interpreter = PDFPageInterpreter(rsrcmgr, device) for page in PDFPage.create_pages(doc): interpreter.process_page(page) text = output_string.getvalue() return re.sub('[\\n\\t\\s]', '', text)  读取docx\nimport docx def read_docx(file): \u0026quot;\u0026quot;\u0026quot; 读取docx文件，并返回其中的文本内容 :param file: docx文件路径 :return: docx中的文本内容 \u0026quot;\u0026quot;\u0026quot; text = '' doc = docx.Document(file) for para in doc.paragraphs: text += para.text return text  \n如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n更多    B站\n  公众号：大邓和他的python\n   知乎\n   github\n​\n  支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1591014680,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1591014680,"objectID":"b3f8f53501c77612df647dac7e88a96f","permalink":"https://thunderhit.github.io/post/pdfdocx/","publishdate":"2020-06-01T20:31:20+08:00","relpermalink":"/post/pdfdocx/","section":"post","summary":"快速构建不同领域(手机、汽车等)的情感词典","tags":["python库","文本分析","事件抽取"],"title":"pdfdocx: 用python读取pdf和docx文件数据","type":"post"},{"authors":["大邓"],"categories":["教程","python库","视频"],"content":"目录    一、简介 二、安装 三、功能说明 四、快速入门  4.1 获取上证交易所上市公司目录 4.2下载某公司所有定期报告文件 4.3 获取某公司的所有定期报告相关信息 4.4 获取某公司的所有定期报告url   五、获取cookies 如果 更多 支持     一、简介 上海证券交易所上市公司定期报告下载,项目地址 https://github.com/thunderhit/shreport   github地址 https://github.com/thunderhit/shreport pypi地址 https://pypi.org/project/shreport  能：\n  获取上证交易所所有公司目录\n  上市公司历年报告(季报、半年报、年报)\n  ​\n 二、安装 pip install shreport  \n三、功能说明 companys() 上证所有上市公司名录，公司名及股票代码 :return: 返回DataFrame pdfurls(code) 仅获取定期报告pdf下载链接 :param code: 股票代码 :return: 年报pdf链接 disclosure(self, code) 获得该公司的股票代码、报告类型、年份、定期报告披露日期、定期报告pdf下载链接, 返回DataFrame :param code: 股票代码 download(code, savepath) 下载该公司（code）的所有季度报告、半年报、年报pdf文件 :param code: 上市公司股票代码 :param savepath: 数据存储所在文件夹的路径，建议使用相对路径  \n四、快速入门 一定要先获得cookies后才能使用下面的所有代码，这里先直接看代码使用情况，cookies获取可见文档\n4.1 获取上证交易所上市公司目录 from shreport import SH cookies = {\u0026quot;Cookie\u0026quot;: '您的cookies'} sh = SH(cookies) df = sh.companys() #将查询结果存储 #df.to_excel('上证交易所上市公司名录.xlsx') #显示前5条数据 df.head()  Run\n   name code     浦发银行 600000   白云机场 600004   东风汽车 600006   中国国贸 600007   首创股份 600008    4.2下载某公司所有定期报告文件 绝大多数报告文件名格式\n   文件 文件名 例子     季度报 公司代码-年份-数字 600000-2000-1.pdf、600000-2000-3.pdf   半年报 公司代码-年份-z 600000-2000-z.pdf   年报 公司代码-年份-n 600000-2000-n.pdf    代码\nfrom pathlib import Path from shreport import SH cookies = {\u0026quot;Cookie\u0026quot;: '您的cookies'} sh = SH(cookies) #获取当前代码所在的文件夹路径 cwd = Path().cwd() #以浦发银行为例股票代码600000 sh.download(code='600000', savepath=cwd)  Run\n=======请耐心等待，正在获取600000数据 =======准备获取600000年报文件链接======== =======年报文件链接已获取完毕============= 已成功下载600000_2000_1.pdf 已成功下载600000_2000_z.pdf 已成功下载600000_2000_3.pdf 已成功下载600000_2000_n.pdf ...... 已成功下载600000_2019_1.pdf 已成功下载600000_2019_z.pdf 已成功下载600000_2019_3.pdf 已成功下载600000_2000_n.pdf  4.3 获取某公司的所有定期报告相关信息 如果暂时不想下载定期报告pdf文件，可以可以先获取某公司的\n 股票代码 报告类型 年份 定期报告披露日期 定期报告pdf下载链接  结果返回DataFrame\nfrom shreport import SH cookies = {\u0026quot;Cookie\u0026quot;: '您的cookies'} sh = SH(cookies) #获取浦发银行披露信息 df = sh.disclosure(code='600000') #存储数据 #df.to_excel('600000.xlsx') #前5条信息 df.head()  Run\n   company code type year date pdf     浦发银行 600000 半年报 2000 2000-07-28 http://www.sse.com.cn/disclosure/listedinfo/announcement/c/600000_2000_1.pdf   浦发银行 600000 第三季度季报 2002 2002-10-30 http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2002-10-30/600000_2002_3.pdf   浦发银行 600000 半年报 2002 2002-08-17 http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2002-08-17/600000_2002_z.pdf   浦发银行 600000 第一季度季报 2002 2002-04-27 http://www.sse.com.cn/disclosure/listedinfo/announcement/c/600000_2002_1.pdf   浦发银行 600000 年报 2001 2002-03-21 http://www.sse.com.cn/disclosure/listedinfo/announcement/c/600000_2001_n.pdf    4.4 获取某公司的所有定期报告url 如果暂时不想下载定期报告pdf文件，可以只得到该公司所有的报告文件链接\nfrom shreport import SH cookies = {\u0026quot;Cookie\u0026quot;: '您的cookies'} sh = SH(cookies) #以浦发银行为例股票代码600000 urls = sh.pdfurls(code='600000') urls  Run\n=======准备获取600000年报文件链接======== =======年报文件链接已获取完毕============= ['http://www.sse.com.cn/disclosure/listedinfo/announcement/c/600000_2000_1.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2002-10-30/600000_2002_3.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2002-08-17/600000_2002_z.pdf', ....... 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/600000_2002_1.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2019-03-26/600000_2018_n.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2018-10-31/600000_2018_3.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2018-08-30/600000_2018_z.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2018-04-28/600000_2017_n.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2018-04-28/600000_2018_1.pdf']  \n五、获取cookies 一定要先获得cookies后才能使用所有的代码，获取方法\n 浏览器访问http://www.sse.com.cn/disclosure/overview/ 按F12（mac按option+command+I)打开开发者工具的Network 刷新网页，耐心寻找与www.sse.com.cn有关的任意网址，找到cookies  \n如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n\n更多   B站 公众号：大邓和他的python  知乎  github  \n支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1590669080,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1590669080,"objectID":"08a33173ec028fe2072d1e6bbf1a4e69","permalink":"https://thunderhit.github.io/post/shreport/","publishdate":"2020-05-28T20:31:20+08:00","relpermalink":"/post/shreport/","section":"post","summary":"目录 一、简介 二、安装 三、功能说明 四、快速入门 4.1 获取上证交易所","tags":["python库","数据采集","网络爬虫"],"title":"shreport: 批量下载上交所定期报告","type":"post"},{"authors":["大邓"],"categories":["教程","python库"],"content":"目录  simtext    安装 使用 参考文献   如果 更多 支持一下     simtext simtext可以计算两文档间四大文本相似性指标，分别为：\n Sim_Cosine cosine相似性 Sim_Jaccard Jaccard相似性 Sim_MinEdit 最小编辑距离 Sim_Simple 微软Word中的track changes  具体算法介绍可翻看Cohen, Lauren, Christopher Malloy\u0026amp;Quoc Nguyen(2018) 第60页\n安装 pip install simtext  使用 中文文本相似性\nfrom simtext import similarity text1 = '在宏观经济背景下，为继续优化贷款结构，重点发展可以抵抗经济周期不良的贷款' text2 = '在宏观经济背景下，为继续优化贷款结构，重点发展可三年专业化、集约化、综合金融+物联网金融四大金融特色的基础上' sim = similarity() res = sim.compute(text1, text2) print(res)  Run\n{'Sim_Cosine': 0.46475800154489, 'Sim_Jaccard': 0.3333333333333333, 'Sim_MinEdit': 29, 'Sim_Simple': 0.9889595182335229}  英文文本相似性\nfrom simtext import similarity A = 'We expect demand to increase.' B = 'We expect worldwide demand to increase.' C = 'We expect weakness in sales' sim = similarity() AB = sim.compute(A, B) AC = sim.compute(A, C) print(AB) print(AC)  Run\n{'Sim_Cosine': 0.9128709291752769, 'Sim_Jaccard': 0.8333333333333334, 'Sim_MinEdit': 2, 'Sim_Simple': 0.9545454545454546} {'Sim_Cosine': 0.39999999999999997, 'Sim_Jaccard': 0.25, 'Sim_MinEdit': 4, 'Sim_Simple': 0.9315789473684211}  参考文献 Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.\n如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n  更多   B站 公众号：大邓和他的python  知乎  github    支持一下 ","date":1590669080,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1590669080,"objectID":"4845bcb71a72e0ae621aac226cf40658","permalink":"https://thunderhit.github.io/post/simtext/","publishdate":"2020-05-28T20:31:20+08:00","relpermalink":"/post/simtext/","section":"post","summary":"目录 simtext 安装 使用 参考文献 如果 更多 支持一下 simtext simtext可以计","tags":["python库","文本分析","文本相似"],"title":"simtext: 计算两文档间四大文本相似性指标","type":"post"},{"authors":["大邓"],"categories":["教程","python库"],"content":"一、cnsenti 中文情感分析库(Chinese Sentiment))可对文本进行情绪分析、正负情感分析。\n   github地址 https://github.com/thunderhit/cnsenti\n   pypi地址 https://pypi.org/project/cnsenti/\n  特性  情感分析默认使用的知网Hownet 情感分析可支持导入自定义txt情感词典(pos和neg) 情绪分析使用大连理工大学情感本体库，可以计算文本中的七大情绪词分布  安装 pip install cnsenti  二、快速上手 中文文本情感词正负情感词统计\nfrom cnsenti import Sentiment senti = Sentiment() test_text= '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心' result = senti.sentiment_count(test_text) print(result)  Run\n{'words': 24, 'sentences': 2, 'pos': 4, 'neg': 0}  中文文本情绪统计\nfrom cnsenti import Emotion emotion = Emotion() test_text = '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心' result = emotion.emotion_count(test_text) print(result)  Run\n{'words': 22, 'sentences': 2, '好': 0, '乐': 4, '哀': 0, '怒': 0, '惧': 0, '恶': 0, '惊': 0}  三、文档 cnsenti包括Emotion和Sentiment两大类，其中\n Emotion 情绪计算类,包括**emotion_count(text)**方法 Sentiment 正负情感计算类，包括**sentiment_count(text)和sentiment_calculate(text)**两种方法  3.1 emotion_count(text) emotion_count(text)y用于统计文本中各种情绪形容词出现的词语数。使用大连理工大学情感本体库词典，支持七种情绪统计(好、乐、哀、怒、惧、恶、惊)。\nfrom cnsenti import Emotion emotion = Emotion() test_text = '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心' result = emotion.emotion_count(test_text) print(result)  返回\n{'words': 22, 'sentences': 2, '好': 0, '乐': 4, '哀': 0, '怒': 0, '惧': 0, '恶': 0, '惊': 0}  其中\n words 中文文本的词语数 sentences 中文文本的句子数 好、乐、哀、怒、惧、恶、惊 text中各自情绪出现的词语数  3.2 sentiment_count(text) 隶属于Sentiment类，可对文本text中的正、负面词进行统计。默认使用Hownet词典，后面会讲到如何导入自定义正、负情感txt词典文件。这里以默认hownet词典进行统计。\nfrom cnsenti import Sentiment senti = Sentiment() test_text = '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心' result = senti.sentiment_count(test_text) print(result)  Run\n{'words': 24, 'sentences': 2, 'pos': 4, 'neg': 0}  其中\n words 文本中词语数 sentences 文本中句子数 pos 文本中正面词总个数 neg 文本中负面词总个数  3.3 sentiment_calculate(text) 隶属于Sentiment类，可更加精准的计算文本的情感信息。相比于sentiment_count只统计文本正负情感词个数，sentiment_calculate还考虑了\n 情感词前是否有强度副词的修饰作用 情感词前是否有否定词的情感语义反转作用  比如\nfrom cnsenti import Sentiment senti = Sentiment() test_text = '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心' result1 = senti.sentiment_count(test_text) result2 = senti.sentiment_calculate(test_text) print('sentiment_count',result1) print('sentiment_calculate',result2)  Run\nsentiment_count {'words': 22, 'sentences': 2, 'pos': 4, 'neg': 0} sentiment_calculate {'sentences': 2, 'words': 22, 'pos': 27.0, 'neg': 0.0}  3.4 自定义词典 我们先看看没有情感形容词的情形\nfrom cnsenti import Sentiment senti = Sentiment() #两txt均为utf-8编码 test_text = '这家公司是行业的引领者，是中流砥柱。' result1 = senti.sentiment_count(test_text) result2 = senti.sentiment_calculate(test_text) print('sentiment_count',result1) print('sentiment_calculate',result2)  Run\nsentiment_count {'words': 10, 'sentences': 1, 'pos': 0, 'neg': 0} sentiment_calculate {'sentences': 1, 'words': 10, 'pos': 0, 'neg': 0}  如我所料，虽然句子是正面的，但是因为cnsenti自带的情感词典仅仅是形容词情感词典，对于很多场景而言，适用性有限，所以pos=0。\n3.4.1 自定词典格式 好在cnsenti支持导入自定义词典，但目前只有Sentiment类支持导入自定义正负情感词典，自定义词典需要满足\n 必须为txt文件 原则上建议encoding为utf-8 txt文件每行只有一个词  3.4.2 Sentiment自定义词典参数 senti = Sentiment(pos='正面词自定义.txt', neg='负面词自定义.txt', merge=True, encoding='utf-8')   pos 正面情感词典txt文件路径 neg 负面情感词典txt文件路径 merge 布尔值；merge=True，cnsenti会融合自定义词典和cnsenti自带词典；merge=False，cnsenti只使用自定义词典 encoding 两txt均为utf-8编码  3.4.3 自定义词典使用案例 这部分我放到test文件夹内,代码和自定义词典均在test内，所以我使用相对路径设定自定义词典的路径\n|test |---代码.py |---正面词自定义.txt |---负面词自定义.txt  正面词自定义.txt\n中流砥柱 引领者  from cnsenti import Sentiment senti = Sentiment(pos='正面词自定义.txt', #正面词典txt文件相对路径 neg='负面词自定义.txt', #负面词典txt文件相对路径 merge=True, #融合cnsenti自带词典和用户导入的自定义词典 encoding='utf-8') #两txt均为utf-8编码 test_text = '这家公司是行业的引领者，是中流砥柱。今年的业绩非常好。' result1 = senti.sentiment_count(test_text) result2 = senti.sentiment_calculate(test_text) print('sentiment_count',result1) print('sentiment_calculate',result2)  Run\nsentiment_count {'words': 16, 'sentences': 2, 'pos': 2, 'neg': 0} sentiment_calculate {'sentences': 2, 'words': 16, 'pos': 5, 'neg': 0}  上面参数我们传入了正面自定义词典和负面自定义词典，并且使用了融合模式（merge=True），可以利用cnsenti自带的词典和刚刚导入的自定义词典进行情感计算。\n补充：\n我设计的这个库目前仅能支持两类型pos和neg，如果你的研究问题是两分类问题，如好坏、美丑、善恶、正邪、友好敌对，你就可以定义两个txt文件，分别赋值给pos和neg，就可以使用cnsenti库。\n四、关于词典 目前比较有可解释性的文本分析方法是词典法，算法逻辑都很清晰。词典的好坏决定了情感分析的好坏。如果没有词典，也就限制了你进行文本情感计算。\n目前大多数人使用的是形容词情感词典，如大连理工大学情感本体库和知网Hownet，优点是直接拿来用，缺点也很明显，对于很多带情感却无形容词的文本无能为力。如这手机很耐摔， 使用形容词情感词典计算得分pos和neg均为0。类似问题在不同研究对象的文本数据应该都是挺普遍的，所以人工构建情感词典还是很有必要的。\n我封装了刘焕勇基于so_pmi算法的新词发现代码，将该库其命名为wordexpansion。wordexpansion可以极大的提高提高自定义词典的构建速度，感兴趣的童鞋详情可以访问 wordexpansion项目地址\n如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n更多   B站 公众号：大邓和他的python  知乎  github  支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1590399080,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1590399080,"objectID":"e80ac415c0bf005ba31960b1948b7e82","permalink":"https://thunderhit.github.io/post/cnsenti/","publishdate":"2020-05-25T17:31:20+08:00","relpermalink":"/post/cnsenti/","section":"post","summary":"快速构建不同领域(手机、汽车等)的情感词典","tags":["python库","文本分析","情感分析"],"title":"cnsenti库:简单易用的中文情感分析库","type":"post"},{"authors":null,"categories":["教程","python库"],"content":"一、cnsenti 中文情感分析库(Chinese Sentiment))可对文本进行情绪分析、正负情感分析。\n   github地址 https://github.com/thunderhit/cnsenti\n   pypi地址 https://pypi.org/project/cnsenti/\n  特性  情感分析默认使用的知网Hownet 情感分析可支持导入自定义txt情感词典(pos和neg) 情绪分析使用大连理工大学情感本体库，可以计算文本中的七大情绪词分布  安装 pip install cnsenti  二、快速上手 中文文本情感词正负情感词统计\nfrom cnsenti import Sentiment senti = Sentiment() test_text= '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心' result = senti.sentiment_count(test_text) print(result)  Run\n{'words': 24, 'sentences': 2, 'pos': 4, 'neg': 0}  中文文本情绪统计\nfrom cnsenti import Emotion emotion = Emotion() test_text = '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心' result = emotion.emotion_count(test_text) print(result)  Run\n{'words': 22, 'sentences': 2, '好': 0, '乐': 4, '哀': 0, '怒': 0, '惧': 0, '恶': 0, '惊': 0}  三、文档 cnsenti包括Emotion和Sentiment两大类，其中\n Emotion 情绪计算类,包括**emotion_count(text)**方法 Sentiment 正负情感计算类，包括**sentiment_count(text)和sentiment_calculate(text)**两种方法  3.1 emotion_count(text) emotion_count(text)y用于统计文本中各种情绪形容词出现的词语数。使用大连理工大学情感本体库词典，支持七种情绪统计(好、乐、哀、怒、惧、恶、惊)。\nfrom cnsenti import Emotion emotion = Emotion() test_text = '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心' result = emotion.emotion_count(test_text) print(result)  返回\n{'words': 22, 'sentences': 2, '好': 0, '乐': 4, '哀': 0, '怒': 0, '惧': 0, '恶': 0, '惊': 0}  其中\n words 中文文本的词语数 sentences 中文文本的句子数 好、乐、哀、怒、惧、恶、惊 text中各自情绪出现的词语数  3.2 sentiment_count(text) 隶属于Sentiment类，可对文本text中的正、负面词进行统计。默认使用Hownet词典，后面会讲到如何导入自定义正、负情感txt词典文件。这里以默认hownet词典进行统计。\nfrom cnsenti import Sentiment senti = Sentiment() test_text = '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心' result = senti.sentiment_count(test_text) print(result)  Run\n{'words': 24, 'sentences': 2, 'pos': 4, 'neg': 0}  其中\n words 文本中词语数 sentences 文本中句子数 pos 文本中正面词总个数 neg 文本中负面词总个数  3.3 sentiment_calculate(text) 隶属于Sentiment类，可更加精准的计算文本的情感信息。相比于sentiment_count只统计文本正负情感词个数，sentiment_calculate还考虑了\n 情感词前是否有强度副词的修饰作用 情感词前是否有否定词的情感语义反转作用  比如\nfrom cnsenti import Sentiment senti = Sentiment() test_text = '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心' result1 = senti.sentiment_count(test_text) result2 = senti.sentiment_calculate(test_text) print('sentiment_count',result1) print('sentiment_calculate',result2)  Run\nsentiment_count {'words': 22, 'sentences': 2, 'pos': 4, 'neg': 0} sentiment_calculate {'sentences': 2, 'words': 22, 'pos': 27.0, 'neg': 0.0}  3.4 自定义词典 我们先看看没有情感形容词的情形\nfrom cnsenti import Sentiment senti = Sentiment() #两txt均为utf-8编码 test_text = '这家公司是行业的引领者，是中流砥柱。' result1 = senti.sentiment_count(test_text) result2 = senti.sentiment_calculate(test_text) print('sentiment_count',result1) print('sentiment_calculate',result2)  Run\nsentiment_count {'words': 10, 'sentences': 1, 'pos': 0, 'neg': 0} sentiment_calculate {'sentences': 1, 'words': 10, 'pos': 0, 'neg': 0}  如我所料，虽然句子是正面的，但是因为cnsenti自带的情感词典仅仅是形容词情感词典，对于很多场景而言，适用性有限，所以pos=0。\n3.4.1 自定词典格式 好在cnsenti支持导入自定义词典，但目前只有Sentiment类支持导入自定义正负情感词典，自定义词典需要满足\n 必须为txt文件 原则上建议encoding为utf-8 txt文件每行只有一个词  3.4.2 Sentiment自定义词典参数 senti = Sentiment(pos='正面词自定义.txt', neg='负面词自定义.txt', merge=True, encoding='utf-8')   pos 正面情感词典txt文件路径 neg 负面情感词典txt文件路径 merge 布尔值；merge=True，cnsenti会融合自定义词典和cnsenti自带词典；merge=False，cnsenti只使用自定义词典 encoding 两txt均为utf-8编码  3.4.3 自定义词典使用案例 这部分我放到test文件夹内,代码和自定义词典均在test内，所以我使用相对路径设定自定义词典的路径\n|test |---代码.py |---正面词自定义.txt |---负面词自定义.txt  正面词自定义.txt\n中流砥柱 引领者  from cnsenti import Sentiment senti = Sentiment(pos='正面词自定义.txt', #正面词典txt文件相对路径 neg='负面词自定义.txt', #负面词典txt文件相对路径 merge=True, #融合cnsenti自带词典和用户导入的自定义词典 encoding='utf-8') #两txt均为utf-8编码 test_text = '这家公司是行业的引领者，是中流砥柱。今年的业绩非常好。' result1 = senti.sentiment_count(test_text) result2 = senti.sentiment_calculate(test_text) print('sentiment_count',result1) print('sentiment_calculate',result2)  Run\nsentiment_count {'words': 16, 'sentences': 2, 'pos': 2, 'neg': 0} sentiment_calculate {'sentences': 2, 'words': 16, 'pos': 5, 'neg': 0}  上面参数我们传入了正面自定义词典和负面自定义词典，并且使用了融合模式（merge=True），可以利用cnsenti自带的词典和刚刚导入的自定义词典进行情感计算。\n补充：\n我设计的这个库目前仅能支持两类型pos和neg，如果你的研究问题是两分类问题，如好坏、美丑、善恶、正邪、友好敌对，你就可以定义两个txt文件，分别赋值给pos和neg，就可以使用cnsenti库。\n四、关于词典 目前比较有可解释性的文本分析方法是词典法，算法逻辑都很清晰。词典的好坏决定了情感分析的好坏。如果没有词典，也就限制了你进行文本情感计算。\n目前大多数人使用的是形容词情感词典，如大连理工大学情感本体库和知网Hownet，优点是直接拿来用，缺点也很明显，对于很多带情感却无形容词的文本无能为力。如这手机很耐摔， 使用形容词情感词典计算得分pos和neg均为0。类似问题在不同研究对象的文本数据应该都是挺普遍的，所以人工构建情感词典还是很有必要的。\n我封装了刘焕勇基于so_pmi算法的新词发现代码，将该库其命名为wordexpansion。wordexpansion可以极大的提高提高自定义词典的构建速度，感兴趣的童鞋详情可以访问 wordexpansion项目地址\n如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n更多   B站 公众号：大邓和他的python  知乎  github  支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1589760000,"objectID":"52fc01deffd9b11f589883567900fec7","permalink":"https://thunderhit.github.io/project/cnsenti/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/project/cnsenti/","section":"project","summary":"快速构建不同领域(手机、汽车等)的情感词典","tags":["python库","文本分析","情感分析"],"title":"cnsenti库","type":"project"},{"authors":null,"categories":["教程","python库"],"content":"一、文本事理类型分析 中文复合事件抽取，可以用来识别文本的模式，包括条件事件、顺承事件、反转事件。\n我仅仅是对代码做了简单的修改，增加了函数说明注释和stats函数，可以用于统计文本中各种模式的分布(数量)情况。代码原作者为刘焕勇 https://github.com/liuhuanyong\n事件图谱（事理图谱）的类型 项目地址https://github.com/liuhuanyong/ComplexEventExtraction 项目介绍很详细，感兴趣的一定要去原项目看一下。\n   事件 含义 形式化 事件应用 图谱场景 举例     条件事件 某事件条件下另一事件发生 如果A那么B 事件预警 时机判定 \u0026lt;限制放宽,立即增产\u0026gt;   反转事件 某事件与另一事件形成对立 虽然A但是B 预防不测 反面教材 \u0026lt;起步晚,发展快\u0026gt;   顺承事件 某事件紧接着另一事件发生 A接着B 事件演化 未来意图识别 \u0026lt;去旅游,买火车票\u0026gt;    分析出文本中的条件、顺承、反转，理论上就可以构建知识网络(本库做不到这可视化)。 1、反转事件图谱 2、条件事件图谱 二、安装方法 pip install eventextraction  三、使用 3.1 主函数 from eventextraction import EventsExtraction extractor = EventsExtraction() content = '虽然你做了坏事，但我觉得你是好人。一旦时机成熟，就坚决推行' datas = extractor.extract_main(content) print(datas)  运行结果\n[{'sent': '虽然你做了坏事，但我觉得你是好人', 'type': 'but', 'tuples': {'pre_wd': '虽然', 'pre_part': '你做了坏事，', 'post_wd': '但', 'post_part ': '我觉得你是好人'}}, {'sent': '一旦时机成熟，就坚决推行', 'type': 'condition', 'tuples': {'pre_wd': '一旦', 'pre_part': '时机成熟，', 'post_wd': '就', 'post_part ': '坚决推行'}}]  3.2 统计 from eventextraction import EventsExtraction extractor = EventsExtraction() content = '虽然你做了坏事，但我觉得你是好人。一旦时机成熟，就坚决推行' datas = extractor.extract_main(content) print(extractor.stats(datas))  运行结果\n{'but': 1, 'condition': 1, 'seq': 0, 'more': 0, 'other': 0}  如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n更多   B站 公众号：大邓和他的python  知乎 [github](https://github.com/thunderhit）  支持 分享不易，谢谢大家点赞分享和红包^_^\n","date":1589587200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1589587200,"objectID":"46ba17014d3db4bcd7e97eedc8d8321b","permalink":"https://thunderhit.github.io/project/eventextraction/","publishdate":"2020-05-16T00:00:00Z","relpermalink":"/project/eventextraction/","section":"project","summary":"快速构建不同领域(手机、汽车等)的情感词典","tags":["python库","文本分析"],"title":"eventextraction库","type":"project"},{"authors":["大邓"],"categories":["教程","python库"],"content":"目录  一、项目意义 二、构建方法 三、安装    2.1 方法一 2.2 加镜像站点 2.3 国内镜像安装     四、使用方法    4.1 文件目录 4.2 构建种子词 4.2 准备发现情感新词 4.3 输出的结果     五、注意：  如果 更多 支持   simtext    安装 使用 参考文献   如果 更多 支持一下      README.md为本人所写，代码底层完全为刘焕勇设计。\n大邓项目地址https://github.com/thunderhit/wordexpansion\n原项目(刘焕勇)地址https://github.com/liuhuanyong/SentimentWordExpansion\n 一、项目意义 情感分析大多是基于情感词典对文本数据进行分析，所以情感词典好坏、是否完备充足是文本分析的关键。\n目前常用的词典都是基于形容词，有\n 知网HowNet 大连理工大学情感本体库  但是形容词类型的词典在某些情况下不适用，比如\n华为手机外壳采用金属制作，更耐摔\n由于句子中没有形容词，使用形容词情感词典计算得到的情感得分为0。但是耐摔这个动词具有正面积极情绪，这个句子的情感的分理应为正\n可见能够简单快速构建不同领域(手机、汽车等)的情感词典十分重要。但是人工构建太慢，如果让机器帮我们把最有可能带情感的候选词找出来，人工再去筛选构建词典，那该多好啊。那么如何构建呢？\n\n二、构建方法 计算机领域有一个算法叫做SO_PMI，互信息。简单的讲个体之间不是完全独立的，往往物以类聚，人以群分。如果我们一开始设定少量的\n 初始正面种子词 初始负面种子词  程序会按照“物以类聚人以群分”的思路，\n 根据初始正面种子词找到很多大概率为正面情感的候选词 根据初始负种子词找到很多大概率为负面情感的候选词  这个包原始作者刘焕勇，项目地址https://github.com/liuhuanyong/SentimentWordExpansion 我仅仅做了简单的封装\n\n三、安装 2.1 方法一 最简单的安装,现在由于国内外网络不稳定，运气不好可能需要尝试几次\npip3 install wordexpansion  2.2 加镜像站点 有的童鞋已经把pip默认安装镜像站点改为国内，如果国内镜像还未收录我的这个包，那么可能会安装失败。只能从国外https://pypi.org/simple站点搜索wordexpansion资源并安装\npip3 install wordexpansion -i https://pypi.org/simple  2.3 国内镜像安装 如果国内镜像站点已经收录，那么使用这个会更快\npip3 install wordexpansion -i https://pypi.tuna.tsinghua.edu.cn/simple/  \n四、使用方法 4.1 文件目录 所有的txt文件，不论输入的还是程序输出的结果，均采用utf-8编码。\n|--test #情感词典扩展与构建测试文件夹 |--find_newwords.py #测试代码 |--test_corpus.txt #语料（某领域）文本数据，5.5M |--test_seed_words.txt #情感种子词，需要手动构建 |--neg_candi.txt #find_newwords.py运行后发现的负面候选词 |--pos_candi.txt #find_newwords.py运行后发现的正面候选词  完整项目请移步至https://github.com/thunderhit/wordexpansion\n4.2 构建种子词 可能我们希望的情感词典几万个，但是种子词100个（正面词50个，负面词50个）说不定就可以。\n手动构建的种子词典test_seed_words.txt(编码encoding为utf-8)中\n 每行一个词 每个词用neg或pos标记 词与标记用空格间隔  休克\tneg 如出一辙\tneg 渴求\tneg 扎堆\tneg 休整\tneg 关门\tneg 阴晴不定\tneg 喜忧参半\tneg 起起伏伏\tneg 一厢情愿\tneg 松紧\tneg 最全\tpos 雄风\tpos 稳健\tpos 稳定\tpos 拉平\tpos 保供\tpos 修正\tpos 稳\tpos 稳住\tpos 保养\tpos ... ...  4.2 准备发现情感新词 已经安装好了wordexpansion，现在我们新建一个名为find_newwords.py的测试代码\n代码中的\nfrom wordexpansion import ChineseSoPmi sopmier = ChineseSoPmi(inputtext_file='test_corpus.txt', seedword_txtfile='test_seed_words.txt', pos_candi_txt_file='pos_candi.txt', neg_candi_txtfile='neg_candi.txt') sopmier.sopmi()  我们的语料数据test_corpus.txt 文件5.5M，100个候选词，运行程序大概耗时60s\n4.3 输出的结果 find_newwords.py运行结束后，会在**同文件夹内(find_newwords.py所在的文件夹)**发现有两个新的txt文件\n pos_candi.txt neg_candi.txt  打开pos_candi.txt, 我们看到\nword,sopmi,polarity,word_length,postag 保持,87.28493062512524,pos,2,v 风险,70.15627986116269,pos,2,n 货币政策,66.28476448498694,pos,4,n 发展,64.40272795986517,pos,2,vn 不要,63.71800916752807,pos,2,df 理念,61.2024367757337,pos,2,n 整体,59.415315156715586,pos,2,n 下,59.321140440512984,pos,1,f 引导,58.5817208758171,pos,2,v 投资,57.71720491331896,pos,2,vn 加强,57.067969337267684,pos,2,v 自己,53.25503772499689,pos,2,r 提升,52.80686380719989,pos,2,v 和,52.12334472663675,pos,1,c 稳步,51.58193211655792,pos,2,d 重要,51.095865548255034,pos,2,a ...  打开neg_candi.txt, 我们看到\nword,sopmi,polarity,word_length,postag 心灵,33.17993872989303,neg,2,n 期间,31.77900620939178,neg,2,f 西溪,30.87839808390589,neg,2,ns 人事,29.594976229171877,neg,2,n 复杂,29.47870186147108,neg,2,a 直到,27.86014637934966,neg,2,v 宰客,27.27304813428452,neg,2,nr 保险,26.433136238404746,neg,2,n 迎来,25.83859896903048,neg,2,v 至少,25.105021416064616,neg,2,d 融资,25.09148586460598,neg,2,vn 或,24.48343281812743,neg,1,c 列,22.20695894382675,neg,1,v 存在,22.041049266517774,neg,2,v ...  从上面的结果看，正面候选词较好，负面候选词有点差强人意。虽然差点，但节约了很多很多时间。\n现在电脑已经帮我们找出候选词，我们人类最擅长找毛病，对neg_candi.txt和pos_candi.txt我们人类只需要一个个挑毛病，把不带正负情感的词剔除掉。这样经过一段时间的剔除工作，针对具体研究领域的专业情感词典就构建出来了。\n\n五、注意：  so_pmi算法效果受训练语料影响，语料规模越大，效果越好 so_pmi算法效率受训练语料影响，语料越大，训练越耗时。100个种子词，5M的数据，大约耗时62.679秒 候选词的选择，可根据PMI值，词长，词性设定规则，进行筛选  所有的txt文件均采用utf-8编码，如果遇到UnicodeDetectorError: \u0026lsquo;gbk\u0026rsquo; codec。。请自行解决文件的encode问题。  如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n更多   B站 公众号：大邓和他的python  知乎 [github](https://github.com/thunderhit/wordexpansion）  支持 分享不易，谢谢大家点赞分享和红包^_^\nsimtext simtext可以计算两文档间四大文本相似性指标，分别为：\n Sim_Cosine cosine相似性 Sim_Jaccard Jaccard相似性 Sim_MinEdit 最小编辑距离 Sim_Simple 微软Word中的track changes  具体算法介绍可翻看Cohen, Lauren, Christopher Malloy\u0026amp;Quoc Nguyen(2018) 第60页\n安装 pip install simtext  使用 中文文本相似性\nfrom simtext import similarity text1 = '在宏观经济背景下，为继续优化贷款结构，重点发展可以抵抗经济周期不良的贷款' text2 = '在宏观经济背景下，为继续优化贷款结构，重点发展可三年专业化、集约化、综合金融+物联网金融四大金融特色的基础上' sim = similarity() res = sim.compute(text1, text2) print(res)  Run\n{'Sim_Cosine': 0.46475800154489, 'Sim_Jaccard': 0.3333333333333333, 'Sim_MinEdit': 29, 'Sim_Simple': 0.9889595182335229}  英文文本相似性\nfrom simtext import similarity A = 'We expect demand to increase.' B = 'We expect worldwide demand to increase.' C = 'We expect weakness in sales' sim = similarity() AB = sim.compute(A, B) AC = sim.compute(A, C) print(AB) print(AC)  Run\n{'Sim_Cosine': 0.9128709291752769, 'Sim_Jaccard': 0.8333333333333334, 'Sim_MinEdit': 2, 'Sim_Simple': 0.9545454545454546} {'Sim_Cosine': 0.39999999999999997, 'Sim_Jaccard': 0.25, 'Sim_MinEdit': 4, 'Sim_Simple': 0.9315789473684211}  参考文献 Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.\n如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n  更多   B站 公众号：大邓和他的python  知乎  github    支持一下 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1589545880,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1589545880,"objectID":"41e33e296d81a88f515e8f5233e0da67","permalink":"https://thunderhit.github.io/post/wordexpansion/","publishdate":"2020-05-15T20:31:20+08:00","relpermalink":"/post/wordexpansion/","section":"post","summary":"目录 一、项目意义 二、构建方法 三、安装 2.1 方法一 2.2 加镜像站点 2.3 国","tags":["python库","文本分析","情感分析"],"title":"wordexpansion: 快速构建不同领域(手机、汽车等)的情感词典","type":"post"},{"authors":null,"categories":["教程","python库","视频"],"content":"cntopic 简单好用的lda话题模型，支持中英文。该库基于gensim和pyLDAvis，实现了lda话题模型及可视化功能。\n \n安装 pip install cntopic  \n使用 这里给大家引入一个场景，假设大家采集新闻数据，忘记采集新闻文本对应的新闻类别，如果人工标注又很费工夫。这时候我们可以用lda话题模型帮我们洞察数据中的规律，发现新闻有n种话题群体。这样lda模型对数据自动打标注topic_1, topic_2, topic_3\u0026hellip; ,topic_n。\n我们研究者的工作量仅仅限于解读topic_1, topic_2, topic_3\u0026hellip; ,topic_n分别是什么话题即可。\nlda训练过程，大致分为\n 读取文件 准备数据 训练lda模型 使用lda模型 存储与导入lda模型  \n1. 读取文件 这里我们用一个新闻数据,一共有10类，每类1000条数据，涵盖\n\u0026lsquo;时尚\u0026rsquo;, \u0026lsquo;财经\u0026rsquo;, \u0026lsquo;科技\u0026rsquo;, \u0026lsquo;教育\u0026rsquo;, \u0026lsquo;家居\u0026rsquo;, \u0026lsquo;体育\u0026rsquo;, \u0026lsquo;时政\u0026rsquo;, \u0026lsquo;游戏\u0026rsquo;, \u0026lsquo;房产\u0026rsquo;, \u0026lsquo;娱乐\u0026rsquo;\nimport pandas as pd df = pd.read_csv('chinese_news.csv') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  label content     0 体育 鲍勃库西奖归谁属？ NCAA最强控卫是坎巴还是弗神新浪体育讯如今，本赛季的NCAA进入到了末...   1 体育 麦基砍28+18+5却充满寂寞 纪录之夜他的痛阿联最懂新浪体育讯上天对每个人都是公平的，贾维...   2 体育 黄蜂vs湖人首发：科比冲击七连胜 火箭两旧将登场新浪体育讯北京时间3月28日，NBA常规赛洛...   3 体育 双面谢亚龙作秀终成做作 谁来为低劣行政能力埋单是谁任命了谢亚龙？谁放纵了谢亚龙？谁又该为谢亚...   4 体育 兔年首战山西换帅后有虎胆 张学文用乔丹名言励志今晚客场挑战浙江稠州银行队，是山西汾酒男篮的兔...     label标签的分布情况\ndf['label'].value_counts()  家居 1000 时尚 1000 房产 1000 时政 1000 教育 1000 游戏 1000 财经 1000 娱乐 1000 体育 1000 科技 1000 Name: label, dtype: int64  \n2. 准备数据 一般准备数据包括:\n 分词、数据清洗 按照模块需求整理数据的格式  注意在scikit-learn中:\n 英文文本不需要分词，原封不动传入即可。 中文文本需要先分词，后整理为英文那样用空格间隔的字符串。形如”我 爱 中国“  import jieba def text2tokens(raw_text): #将文本raw_text分词后得到词语列表 tokens = jieba.lcut(raw_text) #tokens = raw_text.lower().split(' ') #英文用空格分词即可 tokens = [t for t in tokens if len(t)\u0026gt;1] #剔除单字 return tokens #对content列中所有的文本依次进行分词 documents = [text2tokens(txt) for txt in df['content']] #显示前5个document print(documents[:5])  [['鲍勃', '库西', '奖归', 'NCAA', '最强', '控卫', '坎巴', '还是', '弗神', '新浪', '体育讯', '称赞', '得分', '能力', '毋庸置疑',...], ['球员', '大东', '赛区', '锦标赛', '全国', '锦标赛', '他场', '27.1', '6.1', '篮板', '5.1', '助攻',..], ['依旧', '如此', '给力', '疯狂', '表现', '开始', '这个', '赛季', '疯狂', '表现', '结束', '这个', '赛季', '我们', '全国', '锦标赛', '前进', '并且', '之前', '曾经', '连赢', '赢得', '大东', ...], ['赛区', '锦标赛', '冠军', '这些', '归功于', '坎巴', '沃克', '康涅狄格', '大学', '主教练', '吉姆', '卡洪', ...], ['称赞', '一名', '纯正', '控卫', '而且', '能为', '我们', '得分', '单场', '42', '有过', '单场', '17', '助攻', ...]]  \n3. 训练lda模型 现在开始正式使用cntopic模块，开启LDA话题模型分析。步骤包括\n   Step 功能 代码     0 准备documents，已经在前面准备好了 -   1 初始化Topic类 topic = Topic(cwd=os.getcwd())   2 根据documents数据，构建词典空间 topic.create_dictionary(documents=documents)   3 构建语料(将文本转为文档-词频矩阵) topic.create_corpus(documents=documents)   4 指定n_topics，构建LDA话题模型 topic.train_lda_model(n_topics)    这里我们就按照n_topics=10构建lda话题模型，一般情况n_topics可能要实验多次，找到最佳的n_topics\n运行过程中会在代码所在的文件夹内生成一个output文件夹，内部含有\n dictionary.dict 词典文件 lda.model.xxx 多个lda模型文件，其中xxx是代指  上述代码耗时较长，请耐心等待程序运行完毕~\nimport os from cntopic import Topic topic = Topic(cwd=os.getcwd()) #构建词典dictionary topic.create_dictionary(documents=documents) #根据documents数据，构建词典空间 topic.create_corpus(documents=documents) #构建语料(将文本转为文档-词频矩阵) topic.train_lda_model(n_topics=10) #指定n_topics，构建LDA话题模型  \u0026lt;gensim.models.ldamulticore.LdaMulticore at 0x158da5090\u0026gt;  \n4. 使用LDA模型 上面的代码大概运行了5分钟，LDA模型已经训练好了。\n现在我们可以利用LDA做一些事情，包括\n   Step 功能 代码 补充     1 分词后的某文档 document = [\u0026lsquo;游戏\u0026rsquo;, \u0026lsquo;体育\u0026rsquo;]    2 预测document对应的话题 topic.get_document_topics(document)    3 显示每种话题与对应的特征词之间关系 topic.show_topics()    4 数据中不同话题分布情况 topic.topic_distribution(raw_documents) raw_documents是列表或series，如本教程中的df[\u0026lsquo;content\u0026rsquo;]   5 可视化LDA话题模型（功能不稳定） topic.visualize_lda() 可视化结果在output中查找vis.html文件，浏览器打开即可    4.1 准备document 假设有一个文档 '游戏体育真有意思' 分词处理得到document\ndocument = jieba.lcut('游戏体育真有意思') document  ['游戏', '体育', '真', '有意思']  4.2 预测document对应的话题 我们使用topic模型，看看document对应的话题\ntopic.get_document_topics(document)  [(0, 0.02501536), (1, 0.025016038), (2, 0.28541195), (3, 0.025018401), (4, 0.025018891), (5, 0.025017735), (6, 0.51443774), (7, 0.02502284), (8, 0.025015472), (9, 0.025025582)]  我们的lda话题模型是按照n_topics=10训练的，限制调用topic预测某个document时，得到的结果是这10种话题及对应概率的元组列表。\n从中可以看到概率最大的是 话题6， 概率有0.51443774。\n所以我们可以大致认为document是话题6\n4.3 显示每种话题与对应的特征词之间关系 但是仅仅告诉每个文档是 话题n，我们仍然不知道 话题n代表的是什么，所以我们需要看看每种 话题n对应的 特征词语。\ntopic.show_topics()  [(0, '0.042*\u0026quot;基金\u0026quot; + 0.013*\u0026quot;市场\u0026quot; + 0.011*\u0026quot;投资\u0026quot; + 0.009*\u0026quot;公司\u0026quot; + 0.005*\u0026quot;上涨\u0026quot; + 0.004*\u0026quot;股票\u0026quot; + 0.004*\u0026quot;房地产\u0026quot; + 0.004*\u0026quot;指数\u0026quot; + 0.004*\u0026quot;房价\u0026quot; + 0.004*\u0026quot;2008\u0026quot;'), (1, '0.010*\u0026quot;中国\u0026quot; + 0.007*\u0026quot;移民\u0026quot; + 0.006*\u0026quot;项目\u0026quot; + 0.005*\u0026quot;发展\u0026quot; + 0.005*\u0026quot;表示\u0026quot; + 0.005*\u0026quot;经济\u0026quot; + 0.005*\u0026quot;政府\u0026quot; + 0.005*\u0026quot;土地\u0026quot; + 0.004*\u0026quot;政策\u0026quot; + 0.004*\u0026quot;问题\u0026quot;'), (2, '0.014*\u0026quot;比赛\u0026quot; + 0.009*\u0026quot;他们\u0026quot; + 0.008*\u0026quot;球队\u0026quot; + 0.007*\u0026quot;篮板\u0026quot; + 0.006*\u0026quot;我们\u0026quot; + 0.005*\u0026quot;球员\u0026quot; + 0.005*\u0026quot;季后赛\u0026quot; + 0.005*\u0026quot;时间\u0026quot; + 0.005*\u0026quot;热火\u0026quot; + 0.005*\u0026quot;赛季\u0026quot;'), (3, '0.013*\u0026quot;我们\u0026quot; + 0.013*\u0026quot;一个\u0026quot; + 0.009*\u0026quot;自己\u0026quot; + 0.009*\u0026quot;这个\u0026quot; + 0.007*\u0026quot;没有\u0026quot; + 0.007*\u0026quot;他们\u0026quot; + 0.006*\u0026quot;可以\u0026quot; + 0.006*\u0026quot;就是\u0026quot; + 0.006*\u0026quot;很多\u0026quot; + 0.006*\u0026quot;记者\u0026quot;'), (4, '0.020*\u0026quot;电影\u0026quot; + 0.010*\u0026quot;导演\u0026quot; + 0.009*\u0026quot;微博\u0026quot; + 0.008*\u0026quot;影片\u0026quot; + 0.006*\u0026quot;观众\u0026quot; + 0.006*\u0026quot;一个\u0026quot; + 0.005*\u0026quot;自己\u0026quot; + 0.005*\u0026quot;票房\u0026quot; + 0.004*\u0026quot;拍摄\u0026quot; + 0.004*\u0026quot;娱乐\u0026quot;'), (5, '0.018*\u0026quot;学生\u0026quot; + 0.015*\u0026quot;留学\u0026quot; + 0.008*\u0026quot;大学\u0026quot; + 0.008*\u0026quot;可以\u0026quot; + 0.006*\u0026quot;功能\u0026quot; + 0.006*\u0026quot;像素\u0026quot; + 0.006*\u0026quot;拍摄\u0026quot; + 0.006*\u0026quot;采用\u0026quot; + 0.005*\u0026quot;学校\u0026quot; + 0.005*\u0026quot;申请\u0026quot;'), (6, '0.007*\u0026quot;玩家\u0026quot; + 0.006*\u0026quot;封神\u0026quot; + 0.006*\u0026quot;手机\u0026quot; + 0.006*\u0026quot;online\u0026quot; + 0.006*\u0026quot;the\u0026quot; + 0.006*\u0026quot;游戏\u0026quot; + 0.005*\u0026quot;陈水扁\u0026quot; + 0.005*\u0026quot;活动\u0026quot; + 0.005*\u0026quot;to\u0026quot; + 0.005*\u0026quot;一个\u0026quot;'), (7, '0.009*\u0026quot;信息\u0026quot; + 0.009*\u0026quot;考试\u0026quot; + 0.009*\u0026quot;游戏\u0026quot; + 0.007*\u0026quot;工作\u0026quot; + 0.007*\u0026quot;手机\u0026quot; + 0.006*\u0026quot;四六级\u0026quot; + 0.006*\u0026quot;考生\u0026quot; + 0.005*\u0026quot;发展\u0026quot; + 0.004*\u0026quot;可以\u0026quot; + 0.004*\u0026quot;霸王\u0026quot;'), (8, '0.015*\u0026quot;我们\u0026quot; + 0.011*\u0026quot;企业\u0026quot; + 0.011*\u0026quot;产品\u0026quot; + 0.010*\u0026quot;市场\u0026quot; + 0.009*\u0026quot;家具\u0026quot; + 0.009*\u0026quot;品牌\u0026quot; + 0.008*\u0026quot;消费者\u0026quot; + 0.007*\u0026quot;行业\u0026quot; + 0.007*\u0026quot;中国\u0026quot; + 0.007*\u0026quot;一个\u0026quot;'), (9, '0.012*\u0026quot;游戏\u0026quot; + 0.011*\u0026quot;玩家\u0026quot; + 0.010*\u0026quot;可以\u0026quot; + 0.008*\u0026quot;搭配\u0026quot; + 0.008*\u0026quot;活动\u0026quot; + 0.006*\u0026quot;时尚\u0026quot; + 0.005*\u0026quot;OL\u0026quot; + 0.004*\u0026quot;获得\u0026quot; + 0.004*\u0026quot;任务\u0026quot; + 0.004*\u0026quot;手机\u0026quot;')]  根据上面的 话题n 与 特征词 大致可以解读每个 话题n 是什么内容的话题。\n4.4 话题分布情况 现在我们想知道数据集中不同 话题n 的分布情况\ntopic.topic_distribution(raw_documents=df['content'])  9 1670 1 1443 0 1318 5 1265 4 1015 2 970 8 911 3 865 7 307 6 236 Name: topic, dtype: int64  我们的数据有10类，每类是1000条。而现在LDA话题模型单纯的根据文本的一些线索，按照n_topics=10给我们分出的效果还不错。\n最完美的情况是每个 话题n 都是接近1000, 现在 话题9太多， 话题6、 话题7太少。\n不过我们也要注意到某些话题可能存在交集，容易分错，比如\n 财经、房产、时政 体育娱乐 财经、科技  等\n综上，目前模型还算可以，表现还能接受。\n4.5 可视化（功能不稳定） 现在只有10个话题， 我们用肉眼看还能接受，但是当话题数太多的时，还是借助可视化工具帮助我们科学评判训练结果。\n这就用到topic.visualize_lda()，\ntopic.visualize_lda()  运行结束后在\n代码所在的文件夹output文件夹中找vis.html文件，右键浏览器打开。\n可视化功能不稳定，存在vis.html打不开的情况；希望海涵\n图中有左右两大区域\n 左侧 话题分布情况，圆形越大话题越多，圆形四散在四个象限 右侧 某话题对应的特征词，从上到下权重越来越低  需要注意的是左侧\n 尽量圆形均匀分布在四个象限比较好，如果圆形全部集中到有限的区域，模型训练不好 圆形与圆形交集较少比较好，如果交集太多，说明n_topics设置的太大，应该设置的再小一些  \n五、存储与导入lda模型 lda话题模型训练特别慢，如果不保存训练好的模型，实际上是在浪费我们的生命和电脑计算力。\n好消息是cntopic默认为大家存储模型，存储地址是output文件夹内，大家只需要知道如何导入模型即可。\n这里需要导入的有两个模型，使用步骤\n   步骤 模型 代码 作用     0 - - 准备documents   1 - topic = Topic(cwd=os.getcwd()) 初始化   2 词典 topic.load_dictionary(dictpath='output/dictionary.dict\u0026rsquo;) 直接导入词典，省略topic.create_dictionary()   3 - topic.create_corpus(documents=documents) 构建语料(将文本转为文档-词频矩阵)   4 lda话题模型 topic.load_lda_model(modelpath='output/model/lda.model\u0026rsquo;) 导入lda话题模型， 相当于省略topic.train_lda_model(n_topics)    现在我们试一试, 为了与之前的区分，这里我们起名topic2\ntopic2 = Topic(cwd=os.getcwd()) topic2.load_dictionary(dictpath='output/dictionary.dict') topic2.create_corpus(documents=documents) topic2.load_lda_model(modelpath='output/model/lda.model')  大家可以自己回去试一试第4部分使用LDA模型的相关功能\n\n如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n\n更多   B站 公众号：大邓和他的python  知乎  github  \n支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1589500800,"objectID":"555793a0fdd8ec86f082b1b50b352736","permalink":"https://thunderhit.github.io/project/cntopic/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/project/cntopic/","section":"project","summary":"cntopic 简单好用的lda话题模型，支持中英文。该库基于gensim","tags":["python库","文本分析","机器学习","LDA话题模型"],"title":"cntopic:快速构建不同领域(手机、汽车等)的情感词典","type":"project"},{"authors":null,"categories":["教程","python库"],"content":" README.md为本人所写，代码底层完全为刘焕勇设计。\n大邓项目地址https://github.com/thunderhit/wordexpansion\n原项目(刘焕勇)地址https://github.com/liuhuanyong/SentimentWordExpansion\n 一、项目意义 情感分析大多是基于情感词典对文本数据进行分析，所以情感词典好坏、是否完备充足是文本分析的关键。\n目前常用的词典都是基于形容词，有\n 知网HowNet 大连理工大学情感本体库  但是形容词类型的词典在某些情况下不适用，比如\n华为手机外壳采用金属制作，更耐摔\n由于句子中没有形容词，使用形容词情感词典计算得到的情感得分为0。但是耐摔这个动词具有正面积极情绪，这个句子的情感的分理应为正\n可见能够简单快速构建不同领域(手机、汽车等)的情感词典十分重要。但是人工构建太慢，如果让机器帮我们把最有可能带情感的候选词找出来，人工再去筛选构建词典，那该多好啊。那么如何构建呢？\n\n二、构建方法 计算机领域有一个算法叫做SO_PMI，互信息。简单的讲个体之间不是完全独立的，往往物以类聚，人以群分。如果我们一开始设定少量的\n 初始正面种子词 初始负面种子词  程序会按照“物以类聚人以群分”的思路，\n 根据初始正面种子词找到很多大概率为正面情感的候选词 根据初始负种子词找到很多大概率为负面情感的候选词  这个包原始作者刘焕勇，项目地址https://github.com/liuhuanyong/SentimentWordExpansion 我仅仅做了简单的封装\n\n三、安装 2.1 方法一 最简单的安装,现在由于国内外网络不稳定，运气不好可能需要尝试几次\npip3 install wordexpansion  2.2 加镜像站点 有的童鞋已经把pip默认安装镜像站点改为国内，如果国内镜像还未收录我的这个包，那么可能会安装失败。只能从国外https://pypi.org/simple站点搜索wordexpansion资源并安装\npip3 install wordexpansion -i https://pypi.org/simple  2.3 国内镜像安装 如果国内镜像站点已经收录，那么使用这个会更快\npip3 install wordexpansion -i https://pypi.tuna.tsinghua.edu.cn/simple/  \n四、使用方法 4.1 文件目录 所有的txt文件，不论输入的还是程序输出的结果，均采用utf-8编码。\n|--test #情感词典扩展与构建测试文件夹 |--find_newwords.py #测试代码 |--test_corpus.txt #语料（某领域）文本数据，5.5M |--test_seed_words.txt #情感种子词，需要手动构建 |--neg_candi.txt #find_newwords.py运行后发现的负面候选词 |--pos_candi.txt #find_newwords.py运行后发现的正面候选词  完整项目请移步至https://github.com/thunderhit/wordexpansion\n4.2 构建种子词 可能我们希望的情感词典几万个，但是种子词100个（正面词50个，负面词50个）说不定就可以。\n手动构建的种子词典test_seed_words.txt(编码encoding为utf-8)中\n 每行一个词 每个词用neg或pos标记 词与标记用空格间隔  休克\tneg 如出一辙\tneg 渴求\tneg 扎堆\tneg 休整\tneg 关门\tneg 阴晴不定\tneg 喜忧参半\tneg 起起伏伏\tneg 一厢情愿\tneg 松紧\tneg 最全\tpos 雄风\tpos 稳健\tpos 稳定\tpos 拉平\tpos 保供\tpos 修正\tpos 稳\tpos 稳住\tpos 保养\tpos ... ...  4.2 准备发现情感新词 已经安装好了wordexpansion，现在我们新建一个名为find_newwords.py的测试代码\n代码中的\nfrom wordexpansion import ChineseSoPmi sopmier = ChineseSoPmi(inputtext_file='test_corpus.txt', seedword_txtfile='test_seed_words.txt', pos_candi_txt_file='pos_candi.txt', neg_candi_txtfile='neg_candi.txt') sopmier.sopmi()  我们的语料数据test_corpus.txt 文件5.5M，100个候选词，运行程序大概耗时60s\n4.3 输出的结果 find_newwords.py运行结束后，会在**同文件夹内(find_newwords.py所在的文件夹)**发现有两个新的txt文件\n pos_candi.txt neg_candi.txt  打开pos_candi.txt, 我们看到\nword,sopmi,polarity,word_length,postag 保持,87.28493062512524,pos,2,v 风险,70.15627986116269,pos,2,n 货币政策,66.28476448498694,pos,4,n 发展,64.40272795986517,pos,2,vn 不要,63.71800916752807,pos,2,df 理念,61.2024367757337,pos,2,n 整体,59.415315156715586,pos,2,n 下,59.321140440512984,pos,1,f 引导,58.5817208758171,pos,2,v 投资,57.71720491331896,pos,2,vn 加强,57.067969337267684,pos,2,v 自己,53.25503772499689,pos,2,r 提升,52.80686380719989,pos,2,v 和,52.12334472663675,pos,1,c 稳步,51.58193211655792,pos,2,d 重要,51.095865548255034,pos,2,a ...  打开neg_candi.txt, 我们看到\nword,sopmi,polarity,word_length,postag 心灵,33.17993872989303,neg,2,n 期间,31.77900620939178,neg,2,f 西溪,30.87839808390589,neg,2,ns 人事,29.594976229171877,neg,2,n 复杂,29.47870186147108,neg,2,a 直到,27.86014637934966,neg,2,v 宰客,27.27304813428452,neg,2,nr 保险,26.433136238404746,neg,2,n 迎来,25.83859896903048,neg,2,v 至少,25.105021416064616,neg,2,d 融资,25.09148586460598,neg,2,vn 或,24.48343281812743,neg,1,c 列,22.20695894382675,neg,1,v 存在,22.041049266517774,neg,2,v ...  从上面的结果看，正面候选词较好，负面候选词有点差强人意。虽然差点，但节约了很多很多时间。\n现在电脑已经帮我们找出候选词，我们人类最擅长找毛病，对neg_candi.txt和pos_candi.txt我们人类只需要一个个挑毛病，把不带正负情感的词剔除掉。这样经过一段时间的剔除工作，针对具体研究领域的专业情感词典就构建出来了。\n\n五、注意：  so_pmi算法效果受训练语料影响，语料规模越大，效果越好 so_pmi算法效率受训练语料影响，语料越大，训练越耗时。100个种子词，5M的数据，大约耗时62.679秒 候选词的选择，可根据PMI值，词长，词性设定规则，进行筛选  所有的txt文件均采用utf-8编码，如果遇到UnicodeDetectorError: \u0026lsquo;gbk\u0026rsquo; codec。。请自行解决文件的encode问题。  如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n更多   B站 公众号：大邓和他的python  知乎 [github](https://github.com/thunderhit）  支持 分享不易，谢谢大家点赞分享和红包^_^\n","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1589500800,"objectID":"2fd44f15f39393546d03a1b689fdab8b","permalink":"https://thunderhit.github.io/project/wordexpansion/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/project/wordexpansion/","section":"project","summary":"快速构建不同领域(手机、汽车等)的情感词典","tags":["python库","文本分析","情感分析"],"title":"wordexpansion库","type":"project"},{"authors":null,"categories":["教程","python库"],"content":"\n目录  simtext    安装 使用 参考文献   如果 更多 支持     simtext simtext可以计算两文档间四大文本相似性指标，分别为：\n Sim_Cosine cosine相似性 Sim_Jaccard Jaccard相似性 Sim_MinEdit 最小编辑距离 Sim_Simple 微软Word中的track changes  具体算法介绍可翻看Cohen, Lauren, Christopher Malloy\u0026amp;Quoc Nguyen(2018) 第60页\n安装 pip install simtext  使用 中文文本相似性\nfrom simtext import similarity text1 = '在宏观经济背景下，为继续优化贷款结构，重点发展可以抵抗经济周期不良的贷款' text2 = '在宏观经济背景下，为继续优化贷款结构，重点发展可三年专业化、集约化、综合金融+物联网金融四大金融特色的基础上' sim = similarity() res = sim.compute(text1, text2) print(res)  Run\n{'Sim_Cosine': 0.46475800154489, 'Sim_Jaccard': 0.3333333333333333, 'Sim_MinEdit': 29, 'Sim_Simple': 0.9889595182335229}  英文文本相似性\nfrom simtext import similarity A = 'We expect demand to increase.' B = 'We expect worldwide demand to increase.' C = 'We expect weakness in sales' sim = similarity() AB = sim.compute(A, B) AC = sim.compute(A, C) print(AB) print(AC)  Run\n{'Sim_Cosine': 0.9128709291752769, 'Sim_Jaccard': 0.8333333333333334, 'Sim_MinEdit': 2, 'Sim_Simple': 0.9545454545454546} {'Sim_Cosine': 0.39999999999999997, 'Sim_Jaccard': 0.25, 'Sim_MinEdit': 4, 'Sim_Simple': 0.9315789473684211}  参考文献 Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.\n如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n  更多   B站 公众号：大邓和他的python  知乎  github    \n支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1589414400,"objectID":"6e61afc0664181bd27d1f91a49fc5f18","permalink":"https://thunderhit.github.io/project/simtext/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/project/simtext/","section":"project","summary":"计算两文档间四大文本相似性指标","tags":["python库","文本分析","文本相似"],"title":"simtext库","type":"project"},{"authors":null,"categories":["教程","视频"],"content":"目录    一、简介 二、安装 三、功能说明 四、快速入门  4.1 获取上证交易所上市公司目录 4.2下载某公司所有定期报告文件 4.3 获取某公司的所有定期报告相关信息 4.4 获取某公司的所有定期报告url   五、获取cookies 如果 更多 支持     一、简介 上海证券交易所上市公司定期报告下载,项目地址 https://github.com/thunderhit/shreport   github地址 https://github.com/thunderhit/shreport pypi地址 https://pypi.org/project/shreport  能：\n  获取上证交易所所有公司目录\n  上市公司历年报告(季报、半年报、年报)\n   二、安装 pip install shreport  \n三、功能说明 companys() 上证所有上市公司名录，公司名及股票代码 :return: 返回DataFrame pdfurls(code) 仅获取定期报告pdf下载链接 :param code: 股票代码 :return: 年报pdf链接 disclosure(self, code) 获得该公司的股票代码、报告类型、年份、定期报告披露日期、定期报告pdf下载链接, 返回DataFrame :param code: 股票代码 download(code, savepath) 下载该公司（code）的所有季度报告、半年报、年报pdf文件 :param code: 上市公司股票代码 :param savepath: 数据存储所在文件夹的路径，建议使用相对路径  \n四、快速入门 一定要先获得cookies后才能使用下面的所有代码，这里先直接看代码使用情况，cookies获取可见文档\n4.1 获取上证交易所上市公司目录 from shreport import SH cookies = {\u0026quot;Cookie\u0026quot;: '您的cookies'} sh = SH(cookies) df = sh.companys() #将查询结果存储 #df.to_excel('上证交易所上市公司名录.xlsx') #显示前5条数据 df.head()  Run\n   name code     浦发银行 600000   白云机场 600004   东风汽车 600006   中国国贸 600007   首创股份 600008    4.2下载某公司所有定期报告文件 绝大多数报告文件名格式\n   文件 文件名 例子     季度报 公司代码-年份-数字 600000-2000-1.pdf、600000-2000-3.pdf   半年报 公司代码-年份-z 600000-2000-z.pdf   年报 公司代码-年份-n 600000-2000-n.pdf    代码\nfrom pathlib import Path from shreport import SH cookies = {\u0026quot;Cookie\u0026quot;: '您的cookies'} sh = SH(cookies) #获取当前代码所在的文件夹路径 cwd = Path().cwd() #以浦发银行为例股票代码600000 sh.download(code='600000', savepath=cwd)  Run\n=======请耐心等待，正在获取600000数据 =======准备获取600000年报文件链接======== =======年报文件链接已获取完毕============= 已成功下载600000_2000_1.pdf 已成功下载600000_2000_z.pdf 已成功下载600000_2000_3.pdf 已成功下载600000_2000_n.pdf ...... 已成功下载600000_2019_1.pdf 已成功下载600000_2019_z.pdf 已成功下载600000_2019_3.pdf 已成功下载600000_2000_n.pdf  4.3 获取某公司的所有定期报告相关信息 如果暂时不想下载定期报告pdf文件，可以可以先获取某公司的\n 股票代码 报告类型 年份 定期报告披露日期 定期报告pdf下载链接  结果返回DataFrame\nfrom shreport import SH cookies = {\u0026quot;Cookie\u0026quot;: '您的cookies'} sh = SH(cookies) #获取浦发银行披露信息 df = sh.disclosure(code='600000') #存储数据 #df.to_excel('600000.xlsx') #前5条信息 df.head()  Run\n   company code type year date pdf     浦发银行 600000 半年报 2000 2000-07-28 http://www.sse.com.cn/disclosure/listedinfo/announcement/c/600000_2000_1.pdf   浦发银行 600000 第三季度季报 2002 2002-10-30 http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2002-10-30/600000_2002_3.pdf   浦发银行 600000 半年报 2002 2002-08-17 http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2002-08-17/600000_2002_z.pdf   浦发银行 600000 第一季度季报 2002 2002-04-27 http://www.sse.com.cn/disclosure/listedinfo/announcement/c/600000_2002_1.pdf   浦发银行 600000 年报 2001 2002-03-21 http://www.sse.com.cn/disclosure/listedinfo/announcement/c/600000_2001_n.pdf    4.4 获取某公司的所有定期报告url 如果暂时不想下载定期报告pdf文件，可以只得到该公司所有的报告文件链接\nfrom shreport import SH cookies = {\u0026quot;Cookie\u0026quot;: '您的cookies'} sh = SH(cookies) #以浦发银行为例股票代码600000 urls = sh.pdfurls(code='600000') urls  Run\n=======准备获取600000年报文件链接======== =======年报文件链接已获取完毕============= ['http://www.sse.com.cn/disclosure/listedinfo/announcement/c/600000_2000_1.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2002-10-30/600000_2002_3.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2002-08-17/600000_2002_z.pdf', ....... 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/600000_2002_1.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2019-03-26/600000_2018_n.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2018-10-31/600000_2018_3.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2018-08-30/600000_2018_z.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2018-04-28/600000_2017_n.pdf', 'http://www.sse.com.cn/disclosure/listedinfo/announcement/c/2018-04-28/600000_2018_1.pdf']  \n五、获取cookies 一定要先获得cookies后才能使用所有的代码，获取方法\n 浏览器访问http://www.sse.com.cn/disclosure/overview/ 按F12（mac按option+command+I)打开开发者工具的Network 刷新网页，耐心寻找与www.sse.com.cn有关的任意网址，找到cookies  \n如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n\n更多   B站 公众号：大邓和他的python  知乎  github  \n支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1589155200,"objectID":"790a36748482f385377f0596b324ec10","permalink":"https://thunderhit.github.io/project/shreport/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/project/shreport/","section":"project","summary":"批量下载上交所定期报告","tags":["python库","数据采集","网络爬虫"],"title":"shreport库","type":"project"},{"authors":null,"categories":["教程"],"content":"最近运行课件代码，发现pdf文件读取部分的函数失效。这里找到读取pdf文件的可运行代码，为了方便后续学习使用，我已将pdf和docx读取方法封装成pdfdocx包。\npdfdocx github项目地址 只有简单的两个读取函数\n read_pdf(file) read_docx(file)  file为文件路径，函数运行后返回file文件内的文本数据。\n安装 pip install pdfdocx  使用 读取pdf文件\nfrom pdfdocx import read_pdf p_text = read_pdf('test/data.pdf') print(p_text)  Run\n这是来⾃pdf⽂件内的内容  from pdfdocx import read_docx d_text = read_pdf('test/data.docx') print(d_text)  Run\n这是来⾃docx⽂件内的内容  拆开pdfdocx 希望大家能安装好，如果安装或者使用失败，可以使用下面的代码作为备选方法。虽然繁琐，能用就好。\n读取pdf\nfrom io import StringIO from pdfminer.converter import TextConverter from pdfminer.layout import LAParams from pdfminer.pdfdocument import PDFDocument from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter from pdfminer.pdfpage import PDFPage from pdfminer.pdfparser import PDFParser import re def read_pdf(file): \u0026quot;\u0026quot;\u0026quot; 读取pdf文件，并返回其中的文本内容 :param file: pdf文件路径 :return: docx中的文本内容 \u0026quot;\u0026quot;\u0026quot; output_string = StringIO() with open(file, 'rb') as in_file: parser = PDFParser(in_file) doc = PDFDocument(parser) rsrcmgr = PDFResourceManager() device = TextConverter(rsrcmgr, output_string, laparams=LAParams()) interpreter = PDFPageInterpreter(rsrcmgr, device) for page in PDFPage.create_pages(doc): interpreter.process_page(page) text = output_string.getvalue() return re.sub('[\\n\\t\\s]', '', text)  读取docx\nimport docx def read_docx(file): \u0026quot;\u0026quot;\u0026quot; 读取docx文件，并返回其中的文本内容 :param file: docx文件路径 :return: docx中的文本内容 \u0026quot;\u0026quot;\u0026quot; text = '' doc = docx.Document(file) for para in doc.paragraphs: text += para.text return text  \n如果 如果您是经管人文社科专业背景，编程小白，面临海量文本数据采集和处理分析艰巨任务，个人建议学习 《python网络爬虫与文本数据分析》视频课。\n python入门 网络爬虫 数据读取 文本分析入门 机器学习与文本分析 文本分析在经管研究中的应用  感兴趣的童鞋不妨 戳一下 《python网络爬虫与文本数据分析》进来看看~\n更多    B站\n  公众号：大邓和他的python\n   知乎\n   github\n​\n  支持 分享不易，谢谢大家分享（或红包）支持^_^\n","date":1589068800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1589068800,"objectID":"52bd23c6bc58589ec52d044b6f129566","permalink":"https://thunderhit.github.io/project/pdfdocx/","publishdate":"2020-05-10T00:00:00Z","relpermalink":"/project/pdfdocx/","section":"project","summary":"用python读取pdf和docx文件据","tags":["python库","文本分析"],"title":"pdfdocx库","type":"project"},{"authors":["Test"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://thunderhit.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Test"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://thunderhit.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Test"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://thunderhit.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]